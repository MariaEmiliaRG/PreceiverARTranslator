{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install keras_core"
      ],
      "metadata": {
        "id": "21XfrsMF8vpS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bd80edd1-5e75-4ce3-c629-00677394d4de"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting keras_core\n",
            "  Downloading keras_core-0.1.7-py3-none-any.whl (950 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m950.8/950.8 kB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: absl-py in /usr/local/lib/python3.10/dist-packages (from keras_core) (1.4.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from keras_core) (1.25.2)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.10/dist-packages (from keras_core) (13.7.1)\n",
            "Collecting namex (from keras_core)\n",
            "  Downloading namex-0.0.8-py3-none-any.whl (5.8 kB)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.10/dist-packages (from keras_core) (3.9.0)\n",
            "Requirement already satisfied: dm-tree in /usr/local/lib/python3.10/dist-packages (from keras_core) (0.1.8)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich->keras_core) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich->keras_core) (2.16.1)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich->keras_core) (0.1.2)\n",
            "Installing collected packages: namex, keras_core\n",
            "Successfully installed keras_core-0.1.7 namex-0.0.8\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "O00JdIwIRZ_U",
        "tags": [],
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4d50fd80-6519-4aa3-ef27-1bf1d280fea4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using PyTorch backend.\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "os.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"3\" # Disable tensorflow debugging logs\n",
        "os.environ[\"KERAS_BACKEND\"] = \"torch\"\n",
        "import keras_core as keras\n",
        "import tensorflow as tf\n",
        "import random\n",
        "import torch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "tags": [],
        "id": "BfhxWKfjwXMM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c5e40397-063a-440c-c906-61ec4e635dd5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from http://storage.googleapis.com/download.tensorflow.org/data/spa-eng.zip\n",
            "2638744/2638744 [==============================] - 0s 0us/step\n"
          ]
        }
      ],
      "source": [
        "import pathlib\n",
        "\n",
        "path_to_zip = tf.keras.utils.get_file(\n",
        "    \"spa-eng.zip\", origin=\"http://storage.googleapis.com/download.tensorflow.org/data/spa-eng.zip\",\n",
        "    extract=True)\n",
        "path_to_file = pathlib.Path(path_to_zip).parent/\"spa-eng/spa.txt\"\n",
        "\n",
        "with open(path_to_file) as f:\n",
        "    lines = f.read().split(\"\\n\")[:-1]\n",
        "\n",
        "text_pairs = []\n",
        "for line in lines:\n",
        "    eng, spa = line.lower().split(\"\\t\")\n",
        "    text_pairs.append((eng, spa))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "tags": [],
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JiLAXa9twXMN",
        "outputId": "9d9f827f-20c9-47ac-b9ac-e504e5a548e6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "118964 total pairs\n",
            "118370 training pairs\n",
            "594 validation pairs\n"
          ]
        }
      ],
      "source": [
        "random.Random(43).shuffle(text_pairs)\n",
        "num_val_samples = int(0.005 * len(text_pairs))\n",
        "num_train_samples = len(text_pairs) - num_val_samples\n",
        "train_pairs = text_pairs[:num_train_samples]\n",
        "val_pairs = text_pairs[num_train_samples:]\n",
        "\n",
        "print(f\"{len(text_pairs)} total pairs\")\n",
        "print(f\"{len(train_pairs)} training pairs\")\n",
        "print(f\"{len(val_pairs)} validation pairs\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "tags": [],
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TlcqI07mwXMN",
        "outputId": "f5882ccf-8996-40d0-8b0d-1b782dfac823"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "('the old woman fell and could not get up.', 'la anciana se cayó y no pudo levantarse.')\n",
            "('what is this the abbreviation for?', '¿de qué es abreviatura esto?')\n",
            "(\"you're not sick.\", 'no estás enferma.')\n",
            "('i have no knife to cut with.', 'no tengo un cuchillo con que cortarlo.')\n",
            "('americans admire lincoln for his honesty.', 'los estadounidenses admiran a lincoln por su honestidad.')\n"
          ]
        }
      ],
      "source": [
        "for s in train_pairs[:5]:\n",
        "    print(s)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# PIPELINE\n"
      ],
      "metadata": {
        "id": "gzxOciQbeeWy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torchtext.data.utils import get_tokenizer\n",
        "from torchtext.vocab import vocab as Vocab\n",
        "from collections import Counter"
      ],
      "metadata": {
        "id": "k74yIcL9w2c3"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!python -m spacy download en_core_web_sm\n",
        "!python -m spacy download es_core_news_sm"
      ],
      "metadata": {
        "id": "qo7PAIyE9RyR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9f01637a-ae92-4c0d-a7bb-4845319f1bf8"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting en-core-web-sm==3.7.1\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.7.1/en_core_web_sm-3.7.1-py3-none-any.whl (12.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.8/12.8 MB\u001b[0m \u001b[31m20.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: spacy<3.8.0,>=3.7.2 in /usr/local/lib/python3.10/dist-packages (from en-core-web-sm==3.7.1) (3.7.4)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.0.10)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.0.8)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.0.9)\n",
            "Requirement already satisfied: thinc<8.3.0,>=8.2.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (8.2.3)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.1.2)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.4.8)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.0.10)\n",
            "Requirement already satisfied: weasel<0.4.0,>=0.1.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.3.4)\n",
            "Requirement already satisfied: typer<0.10.0,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.9.4)\n",
            "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (6.4.0)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (4.66.2)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.31.0)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.7.0)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.1.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (67.7.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (24.0)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.3.0)\n",
            "Requirement already satisfied: numpy>=1.19.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.25.2)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.6.0)\n",
            "Requirement already satisfied: pydantic-core==2.18.1 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.18.1)\n",
            "Requirement already satisfied: typing-extensions>=4.6.1 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (4.11.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2024.2.2)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.10/dist-packages (from thinc<8.3.0,>=8.2.2->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.7.11)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from thinc<8.3.0,>=8.2.2->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.1.4)\n",
            "Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.10/dist-packages (from typer<0.10.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (8.1.7)\n",
            "Requirement already satisfied: cloudpathlib<0.17.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from weasel<0.4.0,>=0.1.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.16.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.1.5)\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('en_core_web_sm')\n",
            "\u001b[38;5;3m⚠ Restart to reload dependencies\u001b[0m\n",
            "If you are in a Jupyter or Colab notebook, you may need to restart Python in\n",
            "order to load all the package's dependencies. You can do this by selecting the\n",
            "'Restart kernel' or 'Restart runtime' option.\n",
            "Collecting es-core-news-sm==3.7.0\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/es_core_news_sm-3.7.0/es_core_news_sm-3.7.0-py3-none-any.whl (12.9 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.9/12.9 MB\u001b[0m \u001b[31m26.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: spacy<3.8.0,>=3.7.0 in /usr/local/lib/python3.10/dist-packages (from es-core-news-sm==3.7.0) (3.7.4)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (1.0.10)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (2.0.8)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (3.0.9)\n",
            "Requirement already satisfied: thinc<8.3.0,>=8.2.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (8.2.3)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (1.1.2)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (2.4.8)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (2.0.10)\n",
            "Requirement already satisfied: weasel<0.4.0,>=0.1.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (0.3.4)\n",
            "Requirement already satisfied: typer<0.10.0,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (0.9.4)\n",
            "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (6.4.0)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (4.66.2)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (2.31.0)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (2.7.0)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (3.1.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (67.7.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (24.0)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (3.3.0)\n",
            "Requirement already satisfied: numpy>=1.19.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (1.25.2)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (0.6.0)\n",
            "Requirement already satisfied: pydantic-core==2.18.1 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (2.18.1)\n",
            "Requirement already satisfied: typing-extensions>=4.6.1 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (4.11.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (2024.2.2)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.10/dist-packages (from thinc<8.3.0,>=8.2.2->spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (0.7.11)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from thinc<8.3.0,>=8.2.2->spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (0.1.4)\n",
            "Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.10/dist-packages (from typer<0.10.0,>=0.3.0->spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (8.1.7)\n",
            "Requirement already satisfied: cloudpathlib<0.17.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from weasel<0.4.0,>=0.1.0->spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (0.16.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (2.1.5)\n",
            "Installing collected packages: es-core-news-sm\n",
            "Successfully installed es-core-news-sm-3.7.0\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('es_core_news_sm')\n",
            "\u001b[38;5;3m⚠ Restart to reload dependencies\u001b[0m\n",
            "If you are in a Jupyter or Colab notebook, you may need to restart Python in\n",
            "order to load all the package's dependencies. You can do this by selecting the\n",
            "'Restart kernel' or 'Restart runtime' option.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "eng_tokenizer = get_tokenizer(\"spacy\", language=\"en_core_web_sm\")\n",
        "spa_tokenizer = get_tokenizer(\"spacy\", language=\"es_core_news_sm\")"
      ],
      "metadata": {
        "id": "XkaifFNCw7Pu"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def build_vocab(text, tokenizers, min_freq=5):\n",
        "    eng_tokenizer, spa_tokenizer = tokenizers\n",
        "    eng_counter = Counter()\n",
        "    spa_counter = Counter()\n",
        "    for eng_string_, spa_string_ in text:\n",
        "        eng_counter.update(eng_tokenizer(eng_string_))\n",
        "        spa_counter.update(spa_tokenizer(spa_string_))\n",
        "    eng_vocab = Vocab(eng_counter, min_freq=min_freq,\n",
        "                       specials=[\"<unk>\", \"<pad>\"])\n",
        "    spa_vocab = Vocab(spa_counter, min_freq=min_freq,\n",
        "                       specials=[\"<unk>\", \"<pad>\", \"<bos>\", \"<eos>\"])\n",
        "    return eng_vocab, spa_vocab\n",
        "\n",
        "eng_vocab, spa_vocab = build_vocab(text_pairs,\n",
        "                                   [eng_tokenizer, spa_tokenizer],\n",
        "                                   min_freq=0)"
      ],
      "metadata": {
        "id": "w1UVOFChzQxI"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "eng_vocab_size = len(eng_vocab)\n",
        "spa_vocab_size = len(spa_vocab)\n",
        "eng_vocab_size, spa_vocab_size"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aVxbaNuOz5zC",
        "outputId": "144e3dad-51b4-4e01-fa60-fcf950df2a20"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(13229, 26116)"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "maxlen = 20\n",
        "\n",
        "def data_process(text, eng_vocab, spa_vocab, eng_tokenizer, spa_tokenizer):\n",
        "    data = []\n",
        "    for eng, spa in text:\n",
        "        eng_tensor_ = torch.tensor([eng_vocab[token] for token in eng_tokenizer(eng)],\n",
        "                                dtype=torch.long)\n",
        "        spa_tensor_ = torch.tensor([spa_vocab[token] for token in spa_tokenizer(spa)],\n",
        "                                dtype=torch.long)\n",
        "\n",
        "        if eng_tensor_.shape[0] < maxlen and spa_tensor_.shape[0] < maxlen - 2:\n",
        "            data.append((eng_tensor_, spa_tensor_))\n",
        "    return data\n",
        "\n",
        "train_data = data_process(train_pairs, eng_vocab, spa_vocab, eng_tokenizer, spa_tokenizer)\n",
        "val_data = data_process(val_pairs, eng_vocab, spa_vocab, eng_tokenizer, spa_tokenizer)\n",
        "\n",
        "print(len(train_data), len(val_data))"
      ],
      "metadata": {
        "id": "KHnx-TxW5suR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "784a1418-3fb0-4c64-db21-2196ebfda334"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "117552 591\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 64\n",
        "PAD_IDX = eng_vocab[\"<pad>\"]\n",
        "BOS_IDX = spa_vocab[\"<bos>\"]\n",
        "EOS_IDX = spa_vocab[\"<eos>\"]\n",
        "UNK_IDX = spa_vocab[\"<unk>\"]\n",
        "\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.nn import functional as F\n",
        "\n",
        "\n",
        "def pad_to_max_length(tensor, max_length):\n",
        "    current_length = tensor.size(1)\n",
        "    if current_length < max_length:\n",
        "        padding_size = max_length - current_length\n",
        "        padding = torch.full((tensor.size(0), padding_size), PAD_IDX, dtype=torch.long)\n",
        "        tensor = torch.cat([tensor, padding], dim=1)\n",
        "    return tensor\n",
        "\n",
        "def generate_batch(data_batch):\n",
        "    x, y = [], []\n",
        "\n",
        "    for (x_item, y_item) in data_batch:\n",
        "\n",
        "        x.append(torch.cat([x_item]))\n",
        "        y.append(torch.cat([torch.tensor([BOS_IDX]),\n",
        "                            y_item,\n",
        "                            torch.tensor([EOS_IDX])], dim=0))\n",
        "\n",
        "    x = pad_sequence(x, batch_first=True, padding_value=PAD_IDX)\n",
        "    y = pad_sequence(y, batch_first=True, padding_value=PAD_IDX)\n",
        "\n",
        "    # Ajustar a la longitud máxima\n",
        "    x = pad_to_max_length(x, maxlen +2)\n",
        "    y = pad_to_max_length(y, maxlen +2)\n",
        "\n",
        "    return x, y ## Aquí tengo mis dudas si\n"
      ],
      "metadata": {
        "id": "HtirYaUR7rEq"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_loader = DataLoader(train_data, batch_size=batch_size,\n",
        "                          shuffle=True, collate_fn=generate_batch,\n",
        "                          num_workers=4, pin_memory=True)\n",
        "val_loader = DataLoader(val_data, batch_size=batch_size,\n",
        "                          shuffle=True, collate_fn=generate_batch,\n",
        "                          num_workers=4, pin_memory=True)\n",
        "\n"
      ],
      "metadata": {
        "id": "brERdsjtf1tm",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "106acbf0-9fbf-41e1-f310-3b32d47fd754"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:558: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        " train_batch, train_target_batch = next(iter(train_loader))\n",
        " train_batch.shape, train_target_batch.shape\n",
        "\n",
        " print(train_target_batch)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5sqU1qvY8fqc",
        "outputId": "bad30cd1-40d0-4f7e-d4fa-ef8a3e1143b1"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n",
            "  self.pid = os.fork()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[    2,    49,   269,  ...,     1,     1,     1],\n",
            "        [    2,    91, 11680,  ...,     1,     1,     1],\n",
            "        [    2,    13,   513,  ...,     1,     1,     1],\n",
            "        ...,\n",
            "        [    2,    34,  1225,  ...,     1,     1,     1],\n",
            "        [    2,    80,  1847,  ...,     1,     1,     1],\n",
            "        [    2,    22,    26,  ...,     1,     1,     1]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ATENCION"
      ],
      "metadata": {
        "id": "O2xlD_MEg4nD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "from torch import optim\n",
        "import time"
      ],
      "metadata": {
        "id": "w7ZM-KEXAnMg"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Attention(nn.Module): ##SELF ATTENTION\n",
        "    def __init__(self, dim, maxlen, n_heads=4, bias=True):\n",
        "        super().__init__()\n",
        "        self.n_heads = n_heads\n",
        "        self.scale = (dim // n_heads) ** -0.5\n",
        "        self.qw = nn.Linear(dim, dim, bias = bias)\n",
        "        self.kw = nn.Linear(dim, dim, bias = bias)\n",
        "        self.vw = nn.Linear(dim, dim, bias = bias)\n",
        "\n",
        "        self.ow = nn.Linear(dim, dim, bias = bias)\n",
        "        self.register_buffer(\"bias\", torch.tril(torch.ones(maxlen, maxlen)).view(1, 1, maxlen, maxlen))\n",
        "\n",
        "    def forward(self, x):\n",
        "        B, L, D = x.shape\n",
        "        q = self.qw(x)\n",
        "        k = self.kw(x)\n",
        "        v = self.vw(x)\n",
        "        B, L, D = q.shape\n",
        "        q = torch.reshape(q, [B, L, D, -1])\n",
        "        q = torch.permute(q, [0, 2, 1, 3])\n",
        "        k = torch.reshape(k, [B, L, D, -1])\n",
        "        k = torch.permute(k, [0, 2, 3, 1])\n",
        "        v = torch.reshape(v, [B, L, D, -1])\n",
        "        v = torch.permute(v, [0, 2, 1, 3])\n",
        "        qk = torch.matmul(q, k) * self.scale\n",
        "        qk = qk.masked_fill(self.bias[:,:,:L,:L] == 0, float('-inf'))\n",
        "\n",
        "        attn = torch.softmax(qk, dim=-1)\n",
        "\n",
        "        v_attn = torch.matmul(attn, v)\n",
        "        v_attn = torch.permute(v_attn, [0, 2, 1, 3])\n",
        "        v_attn = torch.reshape(v_attn, [B, L, D])\n",
        "\n",
        "        x = self.ow(v_attn)\n",
        "        return x\n",
        "\n",
        "\n",
        "#test_layer = Attention(22, 64, n_heads=1)\n",
        "#test_layer(torch.ones([64,22,22]))"
      ],
      "metadata": {
        "id": "2aUpHphMIntX"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "class CrossAttention(nn.Module): ##CROSS ATTENTION\n",
        "    def __init__(self, dim, maxlen, n_heads=4, bias=True):\n",
        "        super().__init__()\n",
        "        self.n_heads = n_heads\n",
        "        self.scale = (dim // n_heads) ** -0.5\n",
        "        self.qw = nn.Linear(dim, dim, bias=bias)\n",
        "        self.kw = nn.Linear(dim, dim, bias=bias)\n",
        "        self.vw = nn.Linear(dim, dim, bias=bias)\n",
        "\n",
        "        self.ow = nn.Linear(dim, dim, bias=bias)\n",
        "        self.register_buffer(\"bias\", torch.tril(torch.ones(maxlen, maxlen)).view(1, 1, maxlen, maxlen))\n",
        "\n",
        "    def forward(self, x): # k y v son la frase en inglés. Contexto\n",
        "\n",
        "        #print(x.shape)\n",
        "        B, L, D = x.shape\n",
        "        q = self.qw(x)\n",
        "        k = self.kw(x)\n",
        "        v = self.vw(x)\n",
        "\n",
        "        B, L_context, D = k.shape\n",
        "        q = torch.reshape(q, [B, L, self.n_heads, -1])\n",
        "        q = torch.permute(q, [0, 2, 1, 3])\n",
        "        k = torch.reshape(k, [B, L_context, self.n_heads, -1])\n",
        "        k = torch.permute(k, [0, 2, 3, 1])\n",
        "        v = torch.reshape(v, [B, L_context, self.n_heads, -1])\n",
        "        v = torch.permute(v, [0, 2, 1, 3])\n",
        "\n",
        "        qk = torch.matmul(q, k) * self.scale\n",
        "        qk = qk.masked_fill(self.bias[:, :, :L, :L_context] == 0, float('-inf'))\n",
        "\n",
        "        attn = torch.softmax(qk, dim=-1)\n",
        "\n",
        "        v_attn = torch.matmul(attn, v)\n",
        "        v_attn = torch.permute(v_attn, [0, 2, 1, 3])\n",
        "        v_attn = torch.reshape(v_attn, [B, L, D])\n",
        "\n",
        "        x = self.ow(v_attn)\n",
        "        return x\n",
        "\n",
        "test_layer = CrossAttention(32, maxlen + 2, n_heads=1)\n",
        "inputs = torch.ones([1, maxlen, 32])\n",
        "#context = torch.ones([ maxlen-6, 32])\n",
        "#context2 = torch.ones([ maxlen-6, 32])\n",
        "test_layer(inputs)\n"
      ],
      "metadata": {
        "id": "U4M-Ccj8iZie",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3fbabcab-fee1-438a-8c34-6072d99f9d6f"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[[ 0.5545,  0.1373, -0.7807, -0.2249,  0.1230, -0.2947,  0.2303,\n",
              "           0.4823,  0.2490, -0.1458, -0.0849,  0.3339,  0.3349, -0.5101,\n",
              "          -0.0446,  0.2981, -0.0471,  0.3430, -0.1659,  0.1667,  0.7055,\n",
              "          -0.1300, -0.2774,  0.2059,  0.1620, -0.4658, -0.4116, -0.0425,\n",
              "          -0.0114,  0.1036, -0.2742,  0.1340],\n",
              "         [ 0.5545,  0.1373, -0.7807, -0.2249,  0.1230, -0.2947,  0.2303,\n",
              "           0.4823,  0.2490, -0.1458, -0.0849,  0.3339,  0.3349, -0.5101,\n",
              "          -0.0446,  0.2981, -0.0471,  0.3430, -0.1659,  0.1667,  0.7055,\n",
              "          -0.1300, -0.2774,  0.2059,  0.1620, -0.4658, -0.4116, -0.0425,\n",
              "          -0.0114,  0.1036, -0.2742,  0.1340],\n",
              "         [ 0.5545,  0.1373, -0.7807, -0.2249,  0.1230, -0.2947,  0.2303,\n",
              "           0.4823,  0.2490, -0.1458, -0.0849,  0.3339,  0.3349, -0.5101,\n",
              "          -0.0446,  0.2981, -0.0471,  0.3430, -0.1659,  0.1667,  0.7055,\n",
              "          -0.1300, -0.2774,  0.2059,  0.1620, -0.4658, -0.4116, -0.0425,\n",
              "          -0.0114,  0.1036, -0.2742,  0.1340],\n",
              "         [ 0.5545,  0.1373, -0.7807, -0.2249,  0.1230, -0.2947,  0.2303,\n",
              "           0.4823,  0.2490, -0.1458, -0.0849,  0.3339,  0.3349, -0.5101,\n",
              "          -0.0446,  0.2981, -0.0471,  0.3430, -0.1659,  0.1667,  0.7055,\n",
              "          -0.1300, -0.2774,  0.2059,  0.1620, -0.4658, -0.4116, -0.0425,\n",
              "          -0.0114,  0.1036, -0.2742,  0.1340],\n",
              "         [ 0.5545,  0.1373, -0.7807, -0.2249,  0.1230, -0.2947,  0.2303,\n",
              "           0.4823,  0.2490, -0.1458, -0.0849,  0.3339,  0.3349, -0.5101,\n",
              "          -0.0446,  0.2981, -0.0471,  0.3430, -0.1659,  0.1667,  0.7055,\n",
              "          -0.1300, -0.2774,  0.2059,  0.1620, -0.4658, -0.4116, -0.0425,\n",
              "          -0.0114,  0.1036, -0.2742,  0.1340],\n",
              "         [ 0.5545,  0.1373, -0.7807, -0.2249,  0.1230, -0.2947,  0.2303,\n",
              "           0.4823,  0.2490, -0.1458, -0.0849,  0.3339,  0.3349, -0.5101,\n",
              "          -0.0446,  0.2981, -0.0471,  0.3430, -0.1659,  0.1667,  0.7055,\n",
              "          -0.1300, -0.2774,  0.2059,  0.1620, -0.4658, -0.4116, -0.0425,\n",
              "          -0.0114,  0.1036, -0.2742,  0.1340],\n",
              "         [ 0.5545,  0.1373, -0.7807, -0.2249,  0.1230, -0.2947,  0.2303,\n",
              "           0.4823,  0.2490, -0.1458, -0.0849,  0.3339,  0.3349, -0.5101,\n",
              "          -0.0446,  0.2981, -0.0471,  0.3430, -0.1659,  0.1667,  0.7055,\n",
              "          -0.1300, -0.2774,  0.2059,  0.1620, -0.4658, -0.4116, -0.0425,\n",
              "          -0.0114,  0.1036, -0.2742,  0.1340],\n",
              "         [ 0.5545,  0.1373, -0.7807, -0.2249,  0.1230, -0.2947,  0.2303,\n",
              "           0.4823,  0.2490, -0.1458, -0.0849,  0.3339,  0.3349, -0.5101,\n",
              "          -0.0446,  0.2981, -0.0471,  0.3430, -0.1659,  0.1667,  0.7055,\n",
              "          -0.1300, -0.2774,  0.2059,  0.1620, -0.4658, -0.4116, -0.0425,\n",
              "          -0.0114,  0.1036, -0.2742,  0.1340],\n",
              "         [ 0.5545,  0.1373, -0.7807, -0.2249,  0.1230, -0.2947,  0.2303,\n",
              "           0.4823,  0.2490, -0.1458, -0.0849,  0.3339,  0.3349, -0.5101,\n",
              "          -0.0446,  0.2981, -0.0471,  0.3430, -0.1659,  0.1667,  0.7055,\n",
              "          -0.1300, -0.2774,  0.2059,  0.1620, -0.4658, -0.4116, -0.0425,\n",
              "          -0.0114,  0.1036, -0.2742,  0.1340],\n",
              "         [ 0.5545,  0.1373, -0.7807, -0.2249,  0.1230, -0.2947,  0.2303,\n",
              "           0.4823,  0.2490, -0.1458, -0.0849,  0.3339,  0.3349, -0.5101,\n",
              "          -0.0446,  0.2981, -0.0471,  0.3430, -0.1659,  0.1667,  0.7055,\n",
              "          -0.1300, -0.2774,  0.2059,  0.1620, -0.4658, -0.4116, -0.0425,\n",
              "          -0.0114,  0.1036, -0.2742,  0.1340],\n",
              "         [ 0.5545,  0.1373, -0.7807, -0.2249,  0.1230, -0.2947,  0.2303,\n",
              "           0.4823,  0.2490, -0.1458, -0.0849,  0.3339,  0.3349, -0.5101,\n",
              "          -0.0446,  0.2981, -0.0471,  0.3430, -0.1659,  0.1667,  0.7055,\n",
              "          -0.1300, -0.2774,  0.2059,  0.1620, -0.4658, -0.4116, -0.0425,\n",
              "          -0.0114,  0.1036, -0.2742,  0.1340],\n",
              "         [ 0.5545,  0.1373, -0.7807, -0.2249,  0.1230, -0.2947,  0.2303,\n",
              "           0.4823,  0.2490, -0.1458, -0.0849,  0.3339,  0.3349, -0.5101,\n",
              "          -0.0446,  0.2981, -0.0471,  0.3430, -0.1659,  0.1667,  0.7055,\n",
              "          -0.1300, -0.2774,  0.2059,  0.1620, -0.4658, -0.4116, -0.0425,\n",
              "          -0.0114,  0.1036, -0.2742,  0.1340],\n",
              "         [ 0.5545,  0.1373, -0.7807, -0.2249,  0.1230, -0.2947,  0.2303,\n",
              "           0.4823,  0.2490, -0.1458, -0.0849,  0.3339,  0.3349, -0.5101,\n",
              "          -0.0446,  0.2981, -0.0471,  0.3430, -0.1659,  0.1667,  0.7055,\n",
              "          -0.1300, -0.2774,  0.2059,  0.1620, -0.4658, -0.4116, -0.0425,\n",
              "          -0.0114,  0.1036, -0.2742,  0.1340],\n",
              "         [ 0.5545,  0.1373, -0.7807, -0.2249,  0.1230, -0.2947,  0.2303,\n",
              "           0.4823,  0.2490, -0.1458, -0.0849,  0.3339,  0.3349, -0.5101,\n",
              "          -0.0446,  0.2981, -0.0471,  0.3430, -0.1659,  0.1667,  0.7055,\n",
              "          -0.1300, -0.2774,  0.2059,  0.1620, -0.4658, -0.4116, -0.0425,\n",
              "          -0.0114,  0.1036, -0.2742,  0.1340],\n",
              "         [ 0.5545,  0.1373, -0.7807, -0.2249,  0.1230, -0.2947,  0.2303,\n",
              "           0.4823,  0.2490, -0.1458, -0.0849,  0.3339,  0.3349, -0.5101,\n",
              "          -0.0446,  0.2981, -0.0471,  0.3430, -0.1659,  0.1667,  0.7055,\n",
              "          -0.1300, -0.2774,  0.2059,  0.1620, -0.4658, -0.4116, -0.0425,\n",
              "          -0.0114,  0.1036, -0.2742,  0.1340],\n",
              "         [ 0.5545,  0.1373, -0.7807, -0.2249,  0.1230, -0.2947,  0.2303,\n",
              "           0.4823,  0.2490, -0.1458, -0.0849,  0.3339,  0.3349, -0.5101,\n",
              "          -0.0446,  0.2981, -0.0471,  0.3430, -0.1659,  0.1667,  0.7055,\n",
              "          -0.1300, -0.2774,  0.2059,  0.1620, -0.4658, -0.4116, -0.0425,\n",
              "          -0.0114,  0.1036, -0.2742,  0.1340],\n",
              "         [ 0.5545,  0.1373, -0.7807, -0.2249,  0.1230, -0.2947,  0.2303,\n",
              "           0.4823,  0.2490, -0.1458, -0.0849,  0.3339,  0.3349, -0.5101,\n",
              "          -0.0446,  0.2981, -0.0471,  0.3430, -0.1659,  0.1667,  0.7055,\n",
              "          -0.1300, -0.2774,  0.2059,  0.1620, -0.4658, -0.4116, -0.0425,\n",
              "          -0.0114,  0.1036, -0.2742,  0.1340],\n",
              "         [ 0.5545,  0.1373, -0.7807, -0.2249,  0.1230, -0.2947,  0.2303,\n",
              "           0.4823,  0.2490, -0.1458, -0.0849,  0.3339,  0.3349, -0.5101,\n",
              "          -0.0446,  0.2981, -0.0471,  0.3430, -0.1659,  0.1667,  0.7055,\n",
              "          -0.1300, -0.2774,  0.2059,  0.1620, -0.4658, -0.4116, -0.0425,\n",
              "          -0.0114,  0.1036, -0.2742,  0.1340],\n",
              "         [ 0.5545,  0.1373, -0.7807, -0.2249,  0.1230, -0.2947,  0.2303,\n",
              "           0.4823,  0.2490, -0.1458, -0.0849,  0.3339,  0.3349, -0.5101,\n",
              "          -0.0446,  0.2981, -0.0471,  0.3430, -0.1659,  0.1667,  0.7055,\n",
              "          -0.1300, -0.2774,  0.2059,  0.1620, -0.4658, -0.4116, -0.0425,\n",
              "          -0.0114,  0.1036, -0.2742,  0.1340],\n",
              "         [ 0.5545,  0.1373, -0.7807, -0.2249,  0.1230, -0.2947,  0.2303,\n",
              "           0.4823,  0.2490, -0.1458, -0.0849,  0.3339,  0.3349, -0.5101,\n",
              "          -0.0446,  0.2981, -0.0471,  0.3430, -0.1659,  0.1667,  0.7055,\n",
              "          -0.1300, -0.2774,  0.2059,  0.1620, -0.4658, -0.4116, -0.0425,\n",
              "          -0.0114,  0.1036, -0.2742,  0.1340]]], grad_fn=<ViewBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "class CrossAttention3(nn.Module):\n",
        "    def __init__(self, input_dim, context_dim):\n",
        "        super().__init__()\n",
        "        self.q_linear = nn.Linear(input_dim, input_dim)\n",
        "        self.k_linear = nn.Linear(context_dim, input_dim)\n",
        "        self.v_linear = nn.Linear(context_dim, input_dim)\n",
        "        self.out = nn.Linear(input_dim, input_dim)\n",
        "\n",
        "    def forward(self, input_seq, context_seq):\n",
        "        q = self.q_linear(input_seq)\n",
        "        k = self.k_linear(context_seq)\n",
        "        v = self.v_linear(context_seq)\n",
        "\n",
        "        attention_weights = torch.matmul(q, k.transpose(-2, -1))\n",
        "        attention_weights = torch.softmax(attention_weights, dim=-1)\n",
        "\n",
        "        output = torch.matmul(attention_weights, v)\n",
        "        output = self.out(output)\n",
        "\n",
        "        return output\n",
        "\n",
        "# Ejemplo de uso\n",
        "input_dim = 64\n",
        "context_dim = 128\n",
        "max_seq_len = 10\n",
        "\n",
        "cross_attn = CrossAttention3(input_dim, context_dim)\n",
        "input_seq = torch.randn(1, max_seq_len, input_dim)  # Tamaño del lote, longitud de la secuencia, dimensión de entrada\n",
        "context_seq = torch.randn(1, max_seq_len, context_dim)  # Tamaño del lote, longitud de la secuencia, dimensión de contexto\n",
        "\n",
        "output = cross_attn(input_seq, context_seq)\n",
        "print(output.shape)  # Salida: torch.Size([1, 10, 64])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "da3M8CQYI3uJ",
        "outputId": "25bc3c73-e5ba-4179-fa48-a7c24d8c5aaf"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([1, 10, 64])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "UazVmTxOL9HB"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Transformer(nn.Module):\n",
        "    def __init__(self, dim, maxlen, heads=4, mlp_dim=512, rate=0.0):\n",
        "        super().__init__()\n",
        "        #self.ln_1 = nn.LayerNorm(dim)\n",
        "        #self.c_attn = CrossAttention(dim, maxlen)\n",
        "        self.ln_2 = nn.LayerNorm(dim)\n",
        "        self.attn = Attention(dim,maxlen)\n",
        "        self.ln_3 = nn.LayerNorm(dim)\n",
        "        self.mlp = nn.Sequential(\n",
        "            nn.Linear(dim, mlp_dim),\n",
        "            nn.GELU(),\n",
        "            nn.Dropout(rate),\n",
        "            nn.Linear(mlp_dim, dim),\n",
        "            nn.Dropout(rate),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        #x = self.c_attn(self.ln_1(x)) + x\n",
        "        x = self.attn(self.ln_2(x))\n",
        "        return self.mlp(self.ln_3(x))\n",
        "\n",
        "\n",
        "#test_layer = Transformer(32, maxlen + 2)\n",
        "#output = test_layer( torch.ones([1, maxlen + 2, 32]))\n",
        "#print(output.shape)  # Debería imprimir el tamaño del tensor resultant"
      ],
      "metadata": {
        "id": "Pv0_JhpXk0iY"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_batch.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "064vpsAVkAta",
        "outputId": "7eb96585-8de9-466e-cf64-0e2289553de1"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([64, 22])"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import copy\n",
        "\n",
        "class GPT(nn.Module):\n",
        "    def __init__(self, dim, vocab_size_spa, vocab_size_eng, maxlen, depth=3,\n",
        "                 mlp_dim=512, rate=0.2):\n",
        "        super().__init__()\n",
        "        self.embedding_spa = nn.Embedding(vocab_size_spa, dim)\n",
        "        self.pos_embedding_spa = nn.Parameter(\n",
        "            torch.randn(1, maxlen, dim))\n",
        "        self.embedding_eng = nn.Embedding(vocab_size_eng, dim)\n",
        "        self.pos_embedding_eng = nn.Parameter(\n",
        "            torch.randn(1, maxlen, dim))\n",
        "\n",
        "        self.transformer = nn.Sequential()\n",
        "        for _ in range(depth):\n",
        "            self.transformer.append(Transformer(dim, maxlen))\n",
        "\n",
        "        self.head = nn.Linear(dim, vocab_size_spa, bias=False)\n",
        "        self.c_attn = CrossAttention3(dim, dim)\n",
        "\n",
        "\n",
        "    def forward(self, x, y = None):\n",
        "\n",
        "        if y is not None:\n",
        "          self.y = y\n",
        "        #print(\"Hola\")\n",
        "        Bx, Lx = x.shape\n",
        "\n",
        "        context = copy.copy(self.y)\n",
        "        #print(x.shape)\n",
        "        By, Ly = context.reshape(1, -1).shape\n",
        "\n",
        "        x = self.embedding_spa(x)\n",
        "        x += self.pos_embedding_spa[:, :Lx]\n",
        "        context = self.embedding_spa(context)\n",
        "        context += self.pos_embedding_spa[:, :Lx]\n",
        "\n",
        "        #print(x.shape)\n",
        "        x = self.c_attn(x, context)\n",
        "\n",
        "        #print(x.shape)\n",
        "        x = self.transformer(x)\n",
        "        x = self.head(x)\n",
        "        #x = self.fc(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "model_dim = 22\n",
        "depth = 3\n",
        "mlp_dim = 22\n",
        "\n",
        "gpt = GPT(dim=model_dim, vocab_size_spa=spa_vocab_size, vocab_size_eng = eng_vocab_size,\n",
        "          maxlen=maxlen + 2, depth=depth, mlp_dim=mlp_dim)\n",
        "\n",
        "\n",
        "output= gpt(train_batch, train_target_batch)\n",
        "print(output.shape, train_target_batch.shape)\n",
        "\n",
        "train_batch, train_target_batch = next(iter(train_loader))\n",
        "output= gpt(train_batch, train_target_batch)\n",
        "print(output.shape, train_target_batch.shape)"
      ],
      "metadata": {
        "id": "mEbB6pNKmy4s",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d64d31df-b312-428e-943e-6c3b5efb0f39"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([64, 22, 26116]) torch.Size([64, 22])\n",
            "torch.Size([64, 22, 26116]) torch.Size([64, 22])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ENTRENAMIENTO"
      ],
      "metadata": {
        "id": "NHZJwXjFkjNE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "gpt.to(device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I0nM-uUzmsRl",
        "outputId": "15436ac5-840c-40cf-c4af-fb87931122e6"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "GPT(\n",
              "  (embedding_spa): Embedding(26116, 22)\n",
              "  (embedding_eng): Embedding(13229, 22)\n",
              "  (transformer): Sequential(\n",
              "    (0): Transformer(\n",
              "      (ln_2): LayerNorm((22,), eps=1e-05, elementwise_affine=True)\n",
              "      (attn): Attention(\n",
              "        (qw): Linear(in_features=22, out_features=22, bias=True)\n",
              "        (kw): Linear(in_features=22, out_features=22, bias=True)\n",
              "        (vw): Linear(in_features=22, out_features=22, bias=True)\n",
              "        (ow): Linear(in_features=22, out_features=22, bias=True)\n",
              "      )\n",
              "      (ln_3): LayerNorm((22,), eps=1e-05, elementwise_affine=True)\n",
              "      (mlp): Sequential(\n",
              "        (0): Linear(in_features=22, out_features=512, bias=True)\n",
              "        (1): GELU(approximate='none')\n",
              "        (2): Dropout(p=0.0, inplace=False)\n",
              "        (3): Linear(in_features=512, out_features=22, bias=True)\n",
              "        (4): Dropout(p=0.0, inplace=False)\n",
              "      )\n",
              "    )\n",
              "    (1): Transformer(\n",
              "      (ln_2): LayerNorm((22,), eps=1e-05, elementwise_affine=True)\n",
              "      (attn): Attention(\n",
              "        (qw): Linear(in_features=22, out_features=22, bias=True)\n",
              "        (kw): Linear(in_features=22, out_features=22, bias=True)\n",
              "        (vw): Linear(in_features=22, out_features=22, bias=True)\n",
              "        (ow): Linear(in_features=22, out_features=22, bias=True)\n",
              "      )\n",
              "      (ln_3): LayerNorm((22,), eps=1e-05, elementwise_affine=True)\n",
              "      (mlp): Sequential(\n",
              "        (0): Linear(in_features=22, out_features=512, bias=True)\n",
              "        (1): GELU(approximate='none')\n",
              "        (2): Dropout(p=0.0, inplace=False)\n",
              "        (3): Linear(in_features=512, out_features=22, bias=True)\n",
              "        (4): Dropout(p=0.0, inplace=False)\n",
              "      )\n",
              "    )\n",
              "    (2): Transformer(\n",
              "      (ln_2): LayerNorm((22,), eps=1e-05, elementwise_affine=True)\n",
              "      (attn): Attention(\n",
              "        (qw): Linear(in_features=22, out_features=22, bias=True)\n",
              "        (kw): Linear(in_features=22, out_features=22, bias=True)\n",
              "        (vw): Linear(in_features=22, out_features=22, bias=True)\n",
              "        (ow): Linear(in_features=22, out_features=22, bias=True)\n",
              "      )\n",
              "      (ln_3): LayerNorm((22,), eps=1e-05, elementwise_affine=True)\n",
              "      (mlp): Sequential(\n",
              "        (0): Linear(in_features=22, out_features=512, bias=True)\n",
              "        (1): GELU(approximate='none')\n",
              "        (2): Dropout(p=0.0, inplace=False)\n",
              "        (3): Linear(in_features=512, out_features=22, bias=True)\n",
              "        (4): Dropout(p=0.0, inplace=False)\n",
              "      )\n",
              "    )\n",
              "  )\n",
              "  (head): Linear(in_features=22, out_features=26116, bias=False)\n",
              "  (c_attn): CrossAttention3(\n",
              "    (q_linear): Linear(in_features=22, out_features=22, bias=True)\n",
              "    (k_linear): Linear(in_features=22, out_features=22, bias=True)\n",
              "    (v_linear): Linear(in_features=22, out_features=22, bias=True)\n",
              "    (out): Linear(in_features=22, out_features=22, bias=True)\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "PAD_IDX = spa_vocab.get_stoi()['<pad>']\n",
        "PAD_IDX"
      ],
      "metadata": {
        "id": "uSRB-kqQ0Z57",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f720b867-8334-477f-c153-d9b1b1b3f236"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "optimizer = optim.Adam(gpt.parameters(), lr=0.001)\n",
        "def loss_fn(predictions, targets):\n",
        "    # Utiliza torch.nn.CrossEntropyLoss con el índice de ignorancia especificado\n",
        "    loss_fn = torch.nn.CrossEntropyLoss(ignore_index=PAD_IDX)\n",
        "    loss_value = loss_fn(predictions, targets)\n",
        "    return loss_value"
      ],
      "metadata": {
        "id": "mAH0OIf00asK"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train(model, device, train_loader, optimizer, epoch):\n",
        "    start = time.time()\n",
        "    running_loss = 0.0\n",
        "    model.train()\n",
        "\n",
        "    for inputs, targets in train_loader:\n",
        "\n",
        "\n",
        "        inputs, targets = inputs.to(device), targets.to(device)\n",
        "        targets = targets.view(-1)\n",
        "        # zero the parameter gradients\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # forward + backward + optimize\n",
        "        outputs = model(inputs, targets)\n",
        "        outputs = outputs.view(-1, outputs.size(-1))\n",
        "        loss = loss_fn(outputs, targets)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += loss.item()\n",
        "\n",
        "    print(f'\\nTime for epoch {epoch} is {time.time()-start:4f} sec Train loss: {running_loss / len(train_loader):4f}')"
      ],
      "metadata": {
        "id": "7LpeVUem0dZV"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.nn.utils import clip_grad_norm_\n",
        "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
        "\n",
        "def train2(model, device, train_loader, optimizer, epoch, clip_val=5, patience=3):\n",
        "    start = time.time()\n",
        "    running_loss = 0.0\n",
        "    model.train()\n",
        "\n",
        "    scheduler = ReduceLROnPlateau(optimizer, factor=0.1, patience=2)  # Reduce LR by 10% after 2 epochs with no improvement\n",
        "    best_val_loss = float('inf')\n",
        "    current_patience = 0\n",
        "\n",
        "    for inputs, targets in train_loader:\n",
        "        inputs, targets = inputs.to(device), targets.to(device)\n",
        "        #targets = targets.view(-1)\n",
        "        print(inputs.shape, targets.shape)\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(inputs, targets)[:, -1, :]\n",
        "        outputs = outputs.view(-1, outputs.size(-1))\n",
        "\n",
        "        print(outputs.shape, targets.shape)\n",
        "        loss = loss_fn(outputs, targets)\n",
        "        loss.backward()\n",
        "\n",
        "        # Gradient clipping\n",
        "        clip_grad_norm_(model.parameters(), clip_val)\n",
        "        optimizer.step()\n",
        "\n",
        "        #running_loss += loss.item()\n",
        "\n",
        "    print(f'\\nTime for epoch {epoch} is {time.time()-start:4f} sec Train loss: ')#{running_loss / len(train_loader):4f}')\n",
        "\n",
        "    scheduler.step(running_loss / len(train_loader))  # Update scheduler with training loss"
      ],
      "metadata": {
        "id": "H5PhZ4JzeGXU"
      },
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def translate(model, sentence, device, maxlen):\n",
        "    with torch.no_grad():\n",
        "        model.eval()\n",
        "        idx = torch.tensor([eng_vocab[token] for token in eng_tokenizer(sentence)],\n",
        "                                    dtype=torch.long)\n",
        "        idx = idx.reshape([1, -1])\n",
        "        maxlen = maxlen - idx.shape[-1]\n",
        "\n",
        "        for _ in range(maxlen):\n",
        "            idx = idx.to(device)\n",
        "            logits = model(idx, idx)[:, -1, :]\n",
        "            probs = torch.softmax(logits, dim=-1)\n",
        "\n",
        "            _, idx_next = torch.topk(probs, k=1, dim=-1)\n",
        "            idx = torch.cat((idx, idx_next), dim=1)\n",
        "\n",
        "        txt = \" \".join(\n",
        "                    [spa_vocab.get_itos()[idx[0, _]] for _ in range(maxlen)]\n",
        "                )\n",
        "    return txt.replace(\"<eos>\", \"\")\n",
        "\n",
        "sentences = [\"he drinks coffee while reading the newspaper headlines\",\n",
        "             \"families gather for dinner and games\"]\n",
        "\n",
        "for s in sentences:\n",
        "    trans = translate(gpt, s, device, maxlen + 2)\n",
        "    print(f\"\\n{trans}\")"
      ],
      "metadata": {
        "id": "uzv0zMlw0u-6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c4a380af-f81e-4a82-fdf7-3256e6f956f2"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "las grandes bebes sea nosotros <bos> educativo hábitos decides bambú dimitido bambú decides literas\n",
            "\n",
            "verano harvard es veinte se caballo pregúntate pregúntate pregúntate pregúntate pregúntate pregúntate pregúntate enriqueció enriqueció enriqueció\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "epochs = 6\n",
        "\n",
        "for epoch in range(epochs):\n",
        "    train2(gpt, device, train_loader, optimizer, epoch)\n",
        "\n",
        "    # Translate test sentences\n",
        "    for s in sentences:\n",
        "        trans = translate(gpt, s, device, maxlen + 2)\n",
        "        print(s +\" : \"+trans)"
      ],
      "metadata": {
        "id": "M5obVVPTBI-e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 400
        },
        "outputId": "cfe8b3a5-6d67-426b-e3f7-f10b8ea71382"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([64, 22]) torch.Size([64, 22])\n",
            "torch.Size([64, 26116]) torch.Size([64, 22])\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "0D or 1D target tensor expected, multi-target not supported",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-45-8ceffb1a6177>\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0mtrain2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgpt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0;31m# Translate test sentences\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-44-93a6bf1b835c>\u001b[0m in \u001b[0;36mtrain2\u001b[0;34m(model, device, train_loader, optimizer, epoch, clip_val, patience)\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-35-1231324e4237>\u001b[0m in \u001b[0;36mloss_fn\u001b[0;34m(predictions, targets)\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0;31m# Utiliza torch.nn.CrossEntropyLoss con el índice de ignorancia especificado\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mloss_fn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCrossEntropyLoss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mignore_index\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mPAD_IDX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m     \u001b[0mloss_value\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpredictions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mloss_value\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1510\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1511\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1512\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1513\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1518\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1519\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1521\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1522\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/loss.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input, target)\u001b[0m\n\u001b[1;32m   1177\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1178\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1179\u001b[0;31m         return F.cross_entropy(input, target, weight=self.weight,\n\u001b[0m\u001b[1;32m   1180\u001b[0m                                \u001b[0mignore_index\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mignore_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduction\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreduction\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1181\u001b[0m                                label_smoothing=self.label_smoothing)\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mcross_entropy\u001b[0;34m(input, target, weight, size_average, ignore_index, reduce, reduction, label_smoothing)\u001b[0m\n\u001b[1;32m   3057\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0msize_average\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mreduce\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3058\u001b[0m         \u001b[0mreduction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_Reduction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlegacy_get_string\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msize_average\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduce\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3059\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_nn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcross_entropy_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_Reduction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_enum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreduction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mignore_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel_smoothing\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3060\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3061\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: 0D or 1D target tensor expected, multi-target not supported"
          ]
        }
      ]
    }
  ]
}