{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Programa 3\n",
        "### María Emilia Ramírez Gómez"
      ],
      "metadata": {
        "id": "g_iMIp8oR4Sy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install keras_core"
      ],
      "metadata": {
        "id": "21XfrsMF8vpS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7779cde0-dd85-4b6a-bdb1-f602caeafa66"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting keras_core\n",
            "  Downloading keras_core-0.1.7-py3-none-any.whl (950 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m950.8/950.8 kB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: absl-py in /usr/local/lib/python3.10/dist-packages (from keras_core) (1.4.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from keras_core) (1.25.2)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.10/dist-packages (from keras_core) (13.7.1)\n",
            "Collecting namex (from keras_core)\n",
            "  Downloading namex-0.0.8-py3-none-any.whl (5.8 kB)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.10/dist-packages (from keras_core) (3.9.0)\n",
            "Requirement already satisfied: dm-tree in /usr/local/lib/python3.10/dist-packages (from keras_core) (0.1.8)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich->keras_core) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich->keras_core) (2.16.1)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich->keras_core) (0.1.2)\n",
            "Installing collected packages: namex, keras_core\n",
            "Successfully installed keras_core-0.1.7 namex-0.0.8\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "O00JdIwIRZ_U",
        "tags": [],
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5449eb46-1397-4433-f4b3-aac2df47cf39"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using PyTorch backend.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<torch._C.Generator at 0x7d9f5034aeb0>"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ],
      "source": [
        "import os\n",
        "os.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"3\" # Disable tensorflow debugging logs\n",
        "os.environ[\"KERAS_BACKEND\"] = \"torch\"\n",
        "import keras_core as keras\n",
        "import tensorflow as tf\n",
        "import random\n",
        "import torch\n",
        "\n",
        "torch.manual_seed(77)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "tags": [],
        "id": "BfhxWKfjwXMM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b72fb0c7-1dc7-41a3-c892-940d3814ad46"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from http://storage.googleapis.com/download.tensorflow.org/data/spa-eng.zip\n",
            "2638744/2638744 [==============================] - 0s 0us/step\n"
          ]
        }
      ],
      "source": [
        "import pathlib\n",
        "\n",
        "path_to_zip = tf.keras.utils.get_file(\n",
        "    \"spa-eng.zip\", origin=\"http://storage.googleapis.com/download.tensorflow.org/data/spa-eng.zip\",\n",
        "    extract=True)\n",
        "path_to_file = pathlib.Path(path_to_zip).parent/\"spa-eng/spa.txt\"\n",
        "\n",
        "with open(path_to_file) as f:\n",
        "    lines = f.read().split(\"\\n\")[:-1]\n",
        "\n",
        "text_pairs = []\n",
        "for line in lines:\n",
        "    eng, spa = line.lower().split(\"\\t\")\n",
        "    text_pairs.append((eng, spa))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "tags": [],
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JiLAXa9twXMN",
        "outputId": "0f1eaa6d-058c-4b03-fdb8-1e33d3c29748"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "118964 total pairs\n",
            "118370 training pairs\n",
            "594 validation pairs\n"
          ]
        }
      ],
      "source": [
        "random.Random(43).shuffle(text_pairs)\n",
        "num_val_samples = int(0.005 * len(text_pairs))\n",
        "num_train_samples = len(text_pairs) - num_val_samples\n",
        "train_pairs = text_pairs[:num_train_samples]\n",
        "val_pairs = text_pairs[num_train_samples:]\n",
        "\n",
        "print(f\"{len(text_pairs)} total pairs\")\n",
        "print(f\"{len(train_pairs)} training pairs\")\n",
        "print(f\"{len(val_pairs)} validation pairs\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "tags": [],
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TlcqI07mwXMN",
        "outputId": "00e480bf-0091-4a6b-ebdb-be1cf43fcb8e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "('the old woman fell and could not get up.', 'la anciana se cayó y no pudo levantarse.')\n",
            "('what is this the abbreviation for?', '¿de qué es abreviatura esto?')\n",
            "(\"you're not sick.\", 'no estás enferma.')\n",
            "('i have no knife to cut with.', 'no tengo un cuchillo con que cortarlo.')\n",
            "('americans admire lincoln for his honesty.', 'los estadounidenses admiran a lincoln por su honestidad.')\n"
          ]
        }
      ],
      "source": [
        "for s in train_pairs[:5]:\n",
        "    print(s)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# PIPELINE\n"
      ],
      "metadata": {
        "id": "gzxOciQbeeWy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torchtext.data.utils import get_tokenizer\n",
        "from torchtext.vocab import vocab as Vocab\n",
        "from collections import Counter"
      ],
      "metadata": {
        "id": "k74yIcL9w2c3"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!python -m spacy download en_core_web_sm\n",
        "!python -m spacy download es_core_news_sm"
      ],
      "metadata": {
        "id": "qo7PAIyE9RyR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0812f758-f6ff-43e9-9585-b2c02118cc4f"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting en-core-web-sm==3.7.1\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.7.1/en_core_web_sm-3.7.1-py3-none-any.whl (12.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.8/12.8 MB\u001b[0m \u001b[31m26.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: spacy<3.8.0,>=3.7.2 in /usr/local/lib/python3.10/dist-packages (from en-core-web-sm==3.7.1) (3.7.4)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.0.10)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.0.8)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.0.9)\n",
            "Requirement already satisfied: thinc<8.3.0,>=8.2.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (8.2.3)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.1.2)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.4.8)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.0.10)\n",
            "Requirement already satisfied: weasel<0.4.0,>=0.1.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.3.4)\n",
            "Requirement already satisfied: typer<0.10.0,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.9.4)\n",
            "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (6.4.0)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (4.66.2)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.31.0)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.7.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.1.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (67.7.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (24.0)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.4.0)\n",
            "Requirement already satisfied: numpy>=1.19.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.25.2)\n",
            "Requirement already satisfied: language-data>=1.2 in /usr/local/lib/python3.10/dist-packages (from langcodes<4.0.0,>=3.2.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.2.0)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.6.0)\n",
            "Requirement already satisfied: pydantic-core==2.18.2 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.18.2)\n",
            "Requirement already satisfied: typing-extensions>=4.6.1 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (4.11.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2024.2.2)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.10/dist-packages (from thinc<8.3.0,>=8.2.2->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.7.11)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from thinc<8.3.0,>=8.2.2->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.1.4)\n",
            "Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.10/dist-packages (from typer<0.10.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (8.1.7)\n",
            "Requirement already satisfied: cloudpathlib<0.17.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from weasel<0.4.0,>=0.1.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.16.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.1.5)\n",
            "Requirement already satisfied: marisa-trie>=0.7.7 in /usr/local/lib/python3.10/dist-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.1.0)\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('en_core_web_sm')\n",
            "\u001b[38;5;3m⚠ Restart to reload dependencies\u001b[0m\n",
            "If you are in a Jupyter or Colab notebook, you may need to restart Python in\n",
            "order to load all the package's dependencies. You can do this by selecting the\n",
            "'Restart kernel' or 'Restart runtime' option.\n",
            "Collecting es-core-news-sm==3.7.0\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/es_core_news_sm-3.7.0/es_core_news_sm-3.7.0-py3-none-any.whl (12.9 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.9/12.9 MB\u001b[0m \u001b[31m32.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: spacy<3.8.0,>=3.7.0 in /usr/local/lib/python3.10/dist-packages (from es-core-news-sm==3.7.0) (3.7.4)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (1.0.10)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (2.0.8)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (3.0.9)\n",
            "Requirement already satisfied: thinc<8.3.0,>=8.2.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (8.2.3)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (1.1.2)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (2.4.8)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (2.0.10)\n",
            "Requirement already satisfied: weasel<0.4.0,>=0.1.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (0.3.4)\n",
            "Requirement already satisfied: typer<0.10.0,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (0.9.4)\n",
            "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (6.4.0)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (4.66.2)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (2.31.0)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (2.7.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (3.1.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (67.7.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (24.0)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (3.4.0)\n",
            "Requirement already satisfied: numpy>=1.19.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (1.25.2)\n",
            "Requirement already satisfied: language-data>=1.2 in /usr/local/lib/python3.10/dist-packages (from langcodes<4.0.0,>=3.2.0->spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (1.2.0)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (0.6.0)\n",
            "Requirement already satisfied: pydantic-core==2.18.2 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (2.18.2)\n",
            "Requirement already satisfied: typing-extensions>=4.6.1 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (4.11.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (2024.2.2)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.10/dist-packages (from thinc<8.3.0,>=8.2.2->spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (0.7.11)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from thinc<8.3.0,>=8.2.2->spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (0.1.4)\n",
            "Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.10/dist-packages (from typer<0.10.0,>=0.3.0->spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (8.1.7)\n",
            "Requirement already satisfied: cloudpathlib<0.17.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from weasel<0.4.0,>=0.1.0->spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (0.16.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (2.1.5)\n",
            "Requirement already satisfied: marisa-trie>=0.7.7 in /usr/local/lib/python3.10/dist-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (1.1.0)\n",
            "Installing collected packages: es-core-news-sm\n",
            "Successfully installed es-core-news-sm-3.7.0\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('es_core_news_sm')\n",
            "\u001b[38;5;3m⚠ Restart to reload dependencies\u001b[0m\n",
            "If you are in a Jupyter or Colab notebook, you may need to restart Python in\n",
            "order to load all the package's dependencies. You can do this by selecting the\n",
            "'Restart kernel' or 'Restart runtime' option.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "eng_tokenizer = get_tokenizer(\"spacy\", language=\"en_core_web_sm\")\n",
        "spa_tokenizer = get_tokenizer(\"spacy\", language=\"es_core_news_sm\")"
      ],
      "metadata": {
        "id": "XkaifFNCw7Pu"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def build_vocab(text, tokenizers, min_freq=5):\n",
        "    eng_tokenizer, spa_tokenizer = tokenizers\n",
        "    eng_counter = Counter()\n",
        "    spa_counter = Counter()\n",
        "    for eng_string_, spa_string_ in text:\n",
        "        eng_counter.update(eng_tokenizer(eng_string_))\n",
        "        spa_counter.update(spa_tokenizer(spa_string_))\n",
        "    eng_vocab = Vocab(eng_counter, min_freq=min_freq,\n",
        "                       specials=[\"<unk>\", \"<pad>\"])\n",
        "    spa_vocab = Vocab(spa_counter, min_freq=min_freq,\n",
        "                       specials=[\"<unk>\", \"<pad>\", \"<bos>\", \"<eos>\"])\n",
        "    return eng_vocab, spa_vocab\n",
        "\n",
        "eng_vocab, spa_vocab = build_vocab(text_pairs,\n",
        "                                   [eng_tokenizer, spa_tokenizer],\n",
        "                                   min_freq=0)"
      ],
      "metadata": {
        "id": "w1UVOFChzQxI"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "eng_vocab_size = len(eng_vocab)\n",
        "spa_vocab_size = len(spa_vocab)\n",
        "eng_vocab_size, spa_vocab_size"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aVxbaNuOz5zC",
        "outputId": "56fee0d6-fe4e-4614-a6e9-5f2d00944ac8"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(13229, 26116)"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "maxlen = 20\n",
        "\n",
        "def data_process(text, eng_vocab, spa_vocab, eng_tokenizer, spa_tokenizer):\n",
        "    data = []\n",
        "    for eng, spa in text:\n",
        "        eng_tensor_ = torch.tensor([eng_vocab[token] for token in eng_tokenizer(eng)],\n",
        "                                dtype=torch.long)\n",
        "        spa_tensor_ = torch.tensor([spa_vocab[token] for token in spa_tokenizer(spa)],\n",
        "                                dtype=torch.long)\n",
        "\n",
        "        if eng_tensor_.shape[0] < maxlen and spa_tensor_.shape[0] < maxlen - 2:\n",
        "            data.append((eng_tensor_, spa_tensor_))\n",
        "    return data\n",
        "\n",
        "train_data = data_process(train_pairs, eng_vocab, spa_vocab, eng_tokenizer, spa_tokenizer)\n",
        "val_data = data_process(val_pairs, eng_vocab, spa_vocab, eng_tokenizer, spa_tokenizer)\n",
        "\n",
        "print(len(train_data), len(val_data))"
      ],
      "metadata": {
        "id": "KHnx-TxW5suR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ff36c46c-ed32-484c-fd30-e179c2c4f558"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "117552 591\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 64\n",
        "PAD_IDX = eng_vocab[\"<pad>\"]\n",
        "BOS_IDX = spa_vocab[\"<bos>\"]\n",
        "EOS_IDX = spa_vocab[\"<eos>\"]\n",
        "UNK_IDX = spa_vocab[\"<unk>\"]\n",
        "\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.nn import functional as F\n",
        "\n",
        "\n",
        "def pad_to_max_length(tensor, max_length):\n",
        "    current_length = tensor.size(1)\n",
        "    if current_length < max_length:\n",
        "        padding_size = max_length - current_length\n",
        "        padding = torch.full((tensor.size(0), padding_size), PAD_IDX, dtype=torch.long)\n",
        "        tensor = torch.cat([tensor, padding], dim=1)\n",
        "    return tensor\n",
        "\n",
        "def generate_batch(data_batch):\n",
        "    x, y = [], []\n",
        "\n",
        "    for (x_item, y_item) in data_batch:\n",
        "\n",
        "        x.append(torch.cat([x_item]))\n",
        "        y.append(torch.cat([torch.tensor([BOS_IDX]),\n",
        "                            y_item,\n",
        "                            torch.tensor([EOS_IDX])], dim=0))\n",
        "\n",
        "    x = pad_sequence(x, batch_first=True, padding_value=PAD_IDX)\n",
        "    y = pad_sequence(y, batch_first=True, padding_value=PAD_IDX)\n",
        "\n",
        "    # Ajustar a la longitud máxima\n",
        "    x = pad_to_max_length(x, maxlen +2)\n",
        "    y = pad_to_max_length(y, maxlen +2)\n",
        "\n",
        "    return x, y ## Aquí tengo mis dudas si\n"
      ],
      "metadata": {
        "id": "HtirYaUR7rEq"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_loader = DataLoader(train_data, batch_size=batch_size,\n",
        "                          shuffle=True, collate_fn=generate_batch,\n",
        "                          num_workers=4, pin_memory=True)\n",
        "val_loader = DataLoader(val_data, batch_size=batch_size,\n",
        "                          shuffle=True, collate_fn=generate_batch,\n",
        "                          num_workers=4, pin_memory=True)\n",
        "\n"
      ],
      "metadata": {
        "id": "brERdsjtf1tm",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cbd137d0-64b7-49da-96f7-bab5e142a14f"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:558: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        " train_batch, train_target_batch = next(iter(train_loader))\n",
        " train_batch.shape, train_target_batch.shape\n",
        "\n",
        " print(train_target_batch)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5sqU1qvY8fqc",
        "outputId": "49bd4087-5bef-44e9-844b-f176ac710c9f"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n",
            "  self.pid = os.fork()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[   2,  895,  896,  ...,    1,    1,    1],\n",
            "        [   2,  773,  138,  ...,    1,    1,    1],\n",
            "        [   2,    9,  303,  ...,    1,    1,    1],\n",
            "        ...,\n",
            "        [   2,    9,  246,  ...,    1,    1,    1],\n",
            "        [   2, 3928,  185,  ...,    1,    1,    1],\n",
            "        [   2,   81, 5162,  ...,    1,    1,    1]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ATENCION"
      ],
      "metadata": {
        "id": "O2xlD_MEg4nD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "from torch import optim\n",
        "import time"
      ],
      "metadata": {
        "id": "w7ZM-KEXAnMg"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Attention(nn.Module): ##SELF ATTENTION\n",
        "    def __init__(self, dim, maxlen, n_heads=4, bias=True):\n",
        "        super().__init__()\n",
        "        self.n_heads = n_heads\n",
        "        self.scale = (dim // n_heads) ** -0.5\n",
        "        self.qw = nn.Linear(dim, dim, bias = bias)\n",
        "        self.kw = nn.Linear(dim, dim, bias = bias)\n",
        "        self.vw = nn.Linear(dim, dim, bias = bias)\n",
        "\n",
        "        self.ow = nn.Linear(dim, dim, bias = bias)\n",
        "        self.register_buffer(\"bias\", torch.tril(torch.ones(maxlen, maxlen)).view(1, 1, maxlen, maxlen))\n",
        "\n",
        "    def forward(self, x):\n",
        "        B, L, D = x.shape\n",
        "        q = self.qw(x)\n",
        "        k = self.kw(x)\n",
        "        v = self.vw(x)\n",
        "        B, L, D = q.shape\n",
        "        q = torch.reshape(q, [B, L, D, -1])\n",
        "        q = torch.permute(q, [0, 2, 1, 3])\n",
        "        k = torch.reshape(k, [B, L, D, -1])\n",
        "        k = torch.permute(k, [0, 2, 3, 1])\n",
        "        v = torch.reshape(v, [B, L, D, -1])\n",
        "        v = torch.permute(v, [0, 2, 1, 3])\n",
        "        qk = torch.matmul(q, k) * self.scale\n",
        "        qk = qk.masked_fill(self.bias[:,:,:L,:L] == 0, float('-inf'))\n",
        "\n",
        "        attn = torch.softmax(qk, dim=-1)\n",
        "\n",
        "        v_attn = torch.matmul(attn, v)\n",
        "        v_attn = torch.permute(v_attn, [0, 2, 1, 3])\n",
        "        v_attn = torch.reshape(v_attn, [B, L, D])\n",
        "\n",
        "        x = self.ow(v_attn)\n",
        "        return x\n",
        "\n",
        "\n",
        "#test_layer = Attention(22, 64, n_heads=1)\n",
        "#test_layer(torch.ones([64,22,22]))"
      ],
      "metadata": {
        "id": "2aUpHphMIntX"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "class CrossAttention3(nn.Module):\n",
        "    def __init__(self, input_dim, context_dim):\n",
        "        super().__init__()\n",
        "        self.q_linear = nn.Linear(input_dim, input_dim)\n",
        "        self.k_linear = nn.Linear(context_dim // 2, input_dim)\n",
        "        self.v_linear = nn.Linear(context_dim // 2, input_dim)\n",
        "        self.out = nn.Linear(input_dim, input_dim)\n",
        "\n",
        "    def forward(self,  input_seq, context_seq):\n",
        "        q = self.q_linear(input_seq)\n",
        "        context_seq_k, context_seq_v = torch.split(context_seq, context_seq.size(-1) // 2, dim=-1)  # Dividir la secuencia de contexto en K y V\n",
        "\n",
        "        #print(\"k\",context_seq_k.shape, \"v\", context_seq_v.shape )\n",
        "        k = self.k_linear(context_seq_k)\n",
        "        v = self.v_linear(context_seq_v)\n",
        "\n",
        "        attention_weights = torch.matmul(q, k.transpose(-2, -1))\n",
        "        attention_weights = torch.softmax(attention_weights, dim=-1)\n",
        "\n",
        "        output = torch.matmul(attention_weights, v)\n",
        "        output = self.out(output)\n",
        "\n",
        "        return output\n",
        "# Ejemplo de uso:\n",
        "input_dim = 22\n",
        "context_dim = 22\n",
        "input_seq = torch.randn(64, input_dim)\n",
        "context_seq = torch.randn(64, context_dim)\n",
        "\n",
        "model = CrossAttention3(input_dim, context_dim)\n",
        "output = model(input_seq, context_seq)\n",
        "print(output.shape)  #"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "da3M8CQYI3uJ",
        "outputId": "ea1a710f-57b7-4b34-80be-9efdbb4662bb"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([64, 22])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class Transformer(nn.Module):\n",
        "    def __init__(self, dim, maxlen, heads=4, mlp_dim=512, rate=0.2):\n",
        "        super().__init__()\n",
        "        #self.ln_1 = nn.LayerNorm(dim)\n",
        "        #self.c_attn = CrossAttention(dim, maxlen)\n",
        "        self.ln_2 = nn.LayerNorm(dim)\n",
        "        self.attn = Attention(dim,maxlen)\n",
        "        self.ln_3 = nn.LayerNorm(dim)\n",
        "        self.mlp = nn.Sequential(\n",
        "            nn.Linear(dim, mlp_dim),\n",
        "            nn.GELU(),\n",
        "            nn.Dropout(rate),\n",
        "            nn.Linear(mlp_dim, dim),\n",
        "            nn.Dropout(rate),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        #x = self.c_attn(self.ln_1(x)) + x\n",
        "        x = self.attn(self.ln_2(x))\n",
        "        return self.mlp(self.ln_3(x))\n",
        "\n",
        "\n",
        "#test_layer = Transformer(32, maxlen + 2)\n",
        "#output = test_layer( torch.ones([1, maxlen + 2, 32]))\n",
        "#print(output.shape)  # Debería imprimir el tamaño del tensor resultant"
      ],
      "metadata": {
        "id": "Pv0_JhpXk0iY"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_batch.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "064vpsAVkAta",
        "outputId": "22416ba5-cb02-4c06-80a0-59f85b24324e"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([64, 22])"
            ]
          },
          "metadata": {},
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import copy\n",
        "\n",
        "class GPT(nn.Module):\n",
        "    def __init__(self, dim, vocab_size_spa, vocab_size_eng, maxlen, depth=3,\n",
        "                 mlp_dim=512, rate=0.2):\n",
        "        super().__init__()\n",
        "        self.embedding_spa = nn.Embedding(vocab_size_spa, dim)\n",
        "        self.pos_embedding_spa = nn.Parameter(\n",
        "            torch.randn(1, maxlen, dim))\n",
        "        self.embedding_eng = nn.Embedding(vocab_size_eng, dim)\n",
        "        self.pos_embedding_eng = nn.Parameter(\n",
        "            torch.randn(1, maxlen, dim))\n",
        "\n",
        "        self.transformer = nn.Sequential()\n",
        "        for _ in range(depth):\n",
        "            self.transformer.append(Transformer(dim, maxlen))\n",
        "\n",
        "        self.head = nn.Linear(dim, vocab_size_spa, bias=False)\n",
        "        self.c_attn = CrossAttention3(maxlen, maxlen)\n",
        "\n",
        "\n",
        "    def forward(self, x, y = None):\n",
        "\n",
        "        if y is not None:\n",
        "          self.y = y\n",
        "        #print(\"Hola\")\n",
        "        Bx, Lx = x.shape\n",
        "\n",
        "        context = copy.copy(self.y)\n",
        "        #print(x.shape)\n",
        "        #print(self.y.shape)\n",
        "        By, Ly = context.reshape(1, -1).shape\n",
        "\n",
        "        x = self.embedding_spa(x)\n",
        "        x += self.pos_embedding_spa[:, :Lx]\n",
        "        context = self.embedding_spa(context)\n",
        "        context += self.pos_embedding_spa[:, :Lx]\n",
        "\n",
        "        #print(x.shape)\n",
        "        x = self.c_attn(x, context)\n",
        "\n",
        "        #print(x.shape)\n",
        "        x = self.transformer(x)\n",
        "        x = self.head(x)\n",
        "        #x = self.fc(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "model_dim = 22\n",
        "depth = 5\n",
        "mlp_dim = 128\n",
        "\n",
        "gpt = GPT(dim=model_dim, vocab_size_spa=spa_vocab_size, vocab_size_eng = eng_vocab_size,\n",
        "          maxlen=maxlen + 2, depth=depth, mlp_dim=mlp_dim)\n",
        "\n",
        "\n",
        "output= gpt(train_batch, train_target_batch)\n",
        "print(output.shape, train_target_batch.shape)\n",
        "\n",
        "train_batch, train_target_batch = next(iter(train_loader))\n",
        "output= gpt(train_batch, train_target_batch)\n",
        "print(output.shape, train_target_batch.shape)"
      ],
      "metadata": {
        "id": "mEbB6pNKmy4s",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e258be85-8814-4d02-d32b-4593e2ead9e2"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([64, 22, 26116]) torch.Size([64, 22])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:558: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n",
            "/usr/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n",
            "  self.pid = os.fork()\n",
            "/usr/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n",
            "  self.pid = os.fork()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([64, 22, 26116]) torch.Size([64, 22])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ENTRENAMIENTO"
      ],
      "metadata": {
        "id": "NHZJwXjFkjNE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "gpt.to(device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I0nM-uUzmsRl",
        "outputId": "d3f0d2f4-8d84-4573-b213-250f9d274ab7"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "GPT(\n",
              "  (embedding_spa): Embedding(26116, 22)\n",
              "  (embedding_eng): Embedding(13229, 22)\n",
              "  (transformer): Sequential(\n",
              "    (0): Transformer(\n",
              "      (ln_2): LayerNorm((22,), eps=1e-05, elementwise_affine=True)\n",
              "      (attn): Attention(\n",
              "        (qw): Linear(in_features=22, out_features=22, bias=True)\n",
              "        (kw): Linear(in_features=22, out_features=22, bias=True)\n",
              "        (vw): Linear(in_features=22, out_features=22, bias=True)\n",
              "        (ow): Linear(in_features=22, out_features=22, bias=True)\n",
              "      )\n",
              "      (ln_3): LayerNorm((22,), eps=1e-05, elementwise_affine=True)\n",
              "      (mlp): Sequential(\n",
              "        (0): Linear(in_features=22, out_features=512, bias=True)\n",
              "        (1): GELU(approximate='none')\n",
              "        (2): Dropout(p=0.2, inplace=False)\n",
              "        (3): Linear(in_features=512, out_features=22, bias=True)\n",
              "        (4): Dropout(p=0.2, inplace=False)\n",
              "      )\n",
              "    )\n",
              "    (1): Transformer(\n",
              "      (ln_2): LayerNorm((22,), eps=1e-05, elementwise_affine=True)\n",
              "      (attn): Attention(\n",
              "        (qw): Linear(in_features=22, out_features=22, bias=True)\n",
              "        (kw): Linear(in_features=22, out_features=22, bias=True)\n",
              "        (vw): Linear(in_features=22, out_features=22, bias=True)\n",
              "        (ow): Linear(in_features=22, out_features=22, bias=True)\n",
              "      )\n",
              "      (ln_3): LayerNorm((22,), eps=1e-05, elementwise_affine=True)\n",
              "      (mlp): Sequential(\n",
              "        (0): Linear(in_features=22, out_features=512, bias=True)\n",
              "        (1): GELU(approximate='none')\n",
              "        (2): Dropout(p=0.2, inplace=False)\n",
              "        (3): Linear(in_features=512, out_features=22, bias=True)\n",
              "        (4): Dropout(p=0.2, inplace=False)\n",
              "      )\n",
              "    )\n",
              "    (2): Transformer(\n",
              "      (ln_2): LayerNorm((22,), eps=1e-05, elementwise_affine=True)\n",
              "      (attn): Attention(\n",
              "        (qw): Linear(in_features=22, out_features=22, bias=True)\n",
              "        (kw): Linear(in_features=22, out_features=22, bias=True)\n",
              "        (vw): Linear(in_features=22, out_features=22, bias=True)\n",
              "        (ow): Linear(in_features=22, out_features=22, bias=True)\n",
              "      )\n",
              "      (ln_3): LayerNorm((22,), eps=1e-05, elementwise_affine=True)\n",
              "      (mlp): Sequential(\n",
              "        (0): Linear(in_features=22, out_features=512, bias=True)\n",
              "        (1): GELU(approximate='none')\n",
              "        (2): Dropout(p=0.2, inplace=False)\n",
              "        (3): Linear(in_features=512, out_features=22, bias=True)\n",
              "        (4): Dropout(p=0.2, inplace=False)\n",
              "      )\n",
              "    )\n",
              "    (3): Transformer(\n",
              "      (ln_2): LayerNorm((22,), eps=1e-05, elementwise_affine=True)\n",
              "      (attn): Attention(\n",
              "        (qw): Linear(in_features=22, out_features=22, bias=True)\n",
              "        (kw): Linear(in_features=22, out_features=22, bias=True)\n",
              "        (vw): Linear(in_features=22, out_features=22, bias=True)\n",
              "        (ow): Linear(in_features=22, out_features=22, bias=True)\n",
              "      )\n",
              "      (ln_3): LayerNorm((22,), eps=1e-05, elementwise_affine=True)\n",
              "      (mlp): Sequential(\n",
              "        (0): Linear(in_features=22, out_features=512, bias=True)\n",
              "        (1): GELU(approximate='none')\n",
              "        (2): Dropout(p=0.2, inplace=False)\n",
              "        (3): Linear(in_features=512, out_features=22, bias=True)\n",
              "        (4): Dropout(p=0.2, inplace=False)\n",
              "      )\n",
              "    )\n",
              "    (4): Transformer(\n",
              "      (ln_2): LayerNorm((22,), eps=1e-05, elementwise_affine=True)\n",
              "      (attn): Attention(\n",
              "        (qw): Linear(in_features=22, out_features=22, bias=True)\n",
              "        (kw): Linear(in_features=22, out_features=22, bias=True)\n",
              "        (vw): Linear(in_features=22, out_features=22, bias=True)\n",
              "        (ow): Linear(in_features=22, out_features=22, bias=True)\n",
              "      )\n",
              "      (ln_3): LayerNorm((22,), eps=1e-05, elementwise_affine=True)\n",
              "      (mlp): Sequential(\n",
              "        (0): Linear(in_features=22, out_features=512, bias=True)\n",
              "        (1): GELU(approximate='none')\n",
              "        (2): Dropout(p=0.2, inplace=False)\n",
              "        (3): Linear(in_features=512, out_features=22, bias=True)\n",
              "        (4): Dropout(p=0.2, inplace=False)\n",
              "      )\n",
              "    )\n",
              "  )\n",
              "  (head): Linear(in_features=22, out_features=26116, bias=False)\n",
              "  (c_attn): CrossAttention3(\n",
              "    (q_linear): Linear(in_features=22, out_features=22, bias=True)\n",
              "    (k_linear): Linear(in_features=11, out_features=22, bias=True)\n",
              "    (v_linear): Linear(in_features=11, out_features=22, bias=True)\n",
              "    (out): Linear(in_features=22, out_features=22, bias=True)\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "PAD_IDX = spa_vocab.get_stoi()['<pad>']\n",
        "PAD_IDX"
      ],
      "metadata": {
        "id": "uSRB-kqQ0Z57",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d74af573-389d-4a5e-df33-217dc721f4ff"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1"
            ]
          },
          "metadata": {},
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "optimizer = optim.Adam(gpt.parameters(), lr=0.001)\n",
        "loss_fn = torch.nn.CrossEntropyLoss(ignore_index=PAD_IDX)"
      ],
      "metadata": {
        "id": "mAH0OIf00asK"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train(model, device, train_loader, optimizer, epoch):\n",
        "    start = time.time()\n",
        "    running_loss = 0.0\n",
        "    model.train()\n",
        "    for inputs, targets in train_loader:\n",
        "        targets = targets.view(-1)\n",
        "        inputs, targets = inputs.to(device), targets.to(device)\n",
        "\n",
        "        # zero the parameter gradients\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # forward + backward + optimize\n",
        "        outputs = model(inputs)\n",
        "        outputs = outputs.view(-1, outputs.size(-1))\n",
        "        loss = loss_fn(outputs, targets)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += loss.item()\n",
        "\n",
        "    print(f'\\nTime for epoch {epoch} is {time.time()-start:4f} sec Train loss: {running_loss / len(train_loader):4f}')"
      ],
      "metadata": {
        "id": "7LpeVUem0dZV"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.nn.utils import clip_grad_norm_\n",
        "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
        "\n",
        "def train2(model, device, train_loader, optimizer, epoch, clip_val=5, patience=3):\n",
        "    start = time.time()\n",
        "    running_loss = 0.0\n",
        "    model.train()\n",
        "\n",
        "    scheduler = ReduceLROnPlateau(optimizer, factor=0.1, patience=2)  # Reduce LR by 10% after 2 epochs with no improvement\n",
        "    best_val_loss = float('inf')\n",
        "    current_patience = 0\n",
        "\n",
        "    for inputs, targets in train_loader:\n",
        "\n",
        "\n",
        "        inputs, targets = inputs.to(device), targets.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(inputs, targets)\n",
        "\n",
        "        targets = targets.view(-1)\n",
        "        outputs = outputs.view(-1, outputs.size(-1))\n",
        "\n",
        "        loss = loss_fn(outputs, targets)\n",
        "        loss.backward()\n",
        "\n",
        "        # Gradient clipping\n",
        "        clip_grad_norm_(model.parameters(), clip_val)\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += loss.item()\n",
        "\n",
        "    print(f'\\nTime for epoch {epoch} is {time.time()-start:4f} sec Train loss: {running_loss / len(train_loader):4f}')\n",
        "\n",
        "    scheduler.step(running_loss / len(train_loader))  # Update scheduler with training loss"
      ],
      "metadata": {
        "id": "H5PhZ4JzeGXU"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def translate2(model, sentence, device, maxlen):\n",
        "    with torch.no_grad():\n",
        "        model.eval()\n",
        "        idx = torch.tensor([eng_vocab[token] for token in eng_tokenizer(sentence)],\n",
        "                                    dtype=torch.long)\n",
        "        idx = idx.reshape([1, -1])\n",
        "        maxlen = maxlen - idx.shape[-1]\n",
        "\n",
        "        for _ in range(maxlen):\n",
        "            idx = idx.to(device)\n",
        "            logits = model(idx, idx)[:, -1, :]\n",
        "            probs = torch.softmax(logits, dim=-1)\n",
        "\n",
        "            _, idx_next = torch.topk(probs, k=1, dim=-1)\n",
        "            idx = torch.cat((idx, idx_next), dim=1)\n",
        "\n",
        "        txt = \" \".join(\n",
        "                    [spa_vocab.get_itos()[idx[0, _]] for _ in range(maxlen)]\n",
        "                )\n",
        "    return txt.replace(\"<eos>\", \"\")"
      ],
      "metadata": {
        "id": "0HGb4W3i6N_g"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "epochs = 6\n",
        "\n",
        "sentences = [\"he drinks coffee while reading the newspaper headlines\",\n",
        "             \"families gather for dinner and games\"]\n",
        "\n",
        "for epoch in range(epochs):\n",
        "    train2(gpt, device, train_loader, optimizer, epoch)\n",
        "\n",
        "    # Translate test sentences\n",
        "    for s in sentences:\n",
        "        trans = translate2(gpt, s, device, maxlen + 2)\n",
        "        print(s +\" : \"+trans)"
      ],
      "metadata": {
        "id": "M5obVVPTBI-e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "46445260-dfcd-4ba0-f342-e88a94fd0c00"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Time for epoch 0 is 40.068092 sec Train loss: 5.337270\n",
            "he drinks coffee while reading the newspaper headlines : las grandes bebes sea nosotros <bos> educativo hábitos .     \n",
            "families gather for dinner and games : verano harvard es veinte se caballo .  .       \n",
            "\n",
            "Time for epoch 1 is 40.782752 sec Train loss: 5.036201\n",
            "he drinks coffee while reading the newspaper headlines : las grandes bebes sea nosotros <bos> educativo hábitos      \n",
            "families gather for dinner and games : verano harvard es veinte se caballo ¿ ¿ ¿ no no no que es que que\n",
            "\n",
            "Time for epoch 2 is 40.273467 sec Train loss: 4.944837\n",
            "he drinks coffee while reading the newspaper headlines : las grandes bebes sea nosotros <bos> educativo hábitos no es a un . \n",
            "families gather for dinner and games : verano harvard es veinte se caballo es a . . . . . . . \n",
            "\n",
            "Time for epoch 3 is 40.289018 sec Train loss: 4.888206\n",
            "he drinks coffee while reading the newspaper headlines : las grandes bebes sea nosotros <bos> educativo hábitos ¿ no de de de <bos>\n",
            "families gather for dinner and games : verano harvard es veinte se caballo no no que  <bos> <bos> ¿ ¿ ¿ ¿\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# BLEU"
      ],
      "metadata": {
        "id": "Dp5GYhC_Ta9J"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import string\n",
        "import nltk\n",
        "from nltk.translate.bleu_score import sentence_bleu\n",
        "import random\n",
        "\n",
        "\n",
        "def format(s):\n",
        "  tokens = [\"<bos>\", \"<eos>\", \"<unk>\", \"<pad>\"]\n",
        "\n",
        "  for token in tokens:\n",
        "    s = s.replace(token, \"\")\n",
        "\n",
        "  s = s.translate(str.maketrans('', '', string.punctuation + '¡¿'))\n",
        "  s = ' '.join(s.split())\n",
        "  s = s.split()\n",
        "  return s\n",
        "\n",
        "def bleu(val_data):\n",
        "\n",
        "  inputs = [pair[0] for pair in val_pairs]\n",
        "  targets = [pair[1] for pair in val_pairs]\n",
        "\n",
        "  # Get model outputs\n",
        "  outputs = []\n",
        "  for s in inputs:\n",
        "    trans = translate2(gpt, s, device, maxlen)\n",
        "    outputs.append(trans)\n",
        "\n",
        "\n",
        "  targets = [format(s) for s in targets]\n",
        "  outputs = [format(s) for s in outputs]\n",
        "  inputs = [format(s) for s in inputs]\n",
        "\n",
        "  num = 20\n",
        "\n",
        "  for _ in range(num):\n",
        "      ran = random.randint(0, len(targets) - 1)\n",
        "      print(inputs[ran], \"-->\", outputs[ran], \"--\", targets[ran])\n",
        "\n",
        "\n",
        "  # Compute BLEU\n",
        "  score = 0\n",
        "  for i in range(len(output)):\n",
        "    t = targets[i]\n",
        "    o = outputs[i]\n",
        "\n",
        "    bleu_score = sentence_bleu([t], o)\n",
        "    score += bleu_score\n",
        "\n",
        "  print(f'BLEU score promedio: {score/len(output)}')\n",
        "\n",
        "bleu(val_pairs)"
      ],
      "metadata": {
        "id": "FZpHLt4bEtgB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# CONCLUSIÓN"
      ],
      "metadata": {
        "id": "NIRFLViCTKE0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Se intentó implementar una arquitectura perceiver ar para realizar traducciones de inglés a español.\n",
        "\n"
      ],
      "metadata": {
        "id": "i8k0rfqLTMPB"
      }
    }
  ]
}