{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install keras_core"
      ],
      "metadata": {
        "id": "21XfrsMF8vpS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e6fa21a5-7c89-4858-d342-2b8d6aad59e9"
      },
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: keras_core in /usr/local/lib/python3.10/dist-packages (0.1.7)\n",
            "Requirement already satisfied: absl-py in /usr/local/lib/python3.10/dist-packages (from keras_core) (1.4.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from keras_core) (1.25.2)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.10/dist-packages (from keras_core) (13.7.1)\n",
            "Requirement already satisfied: namex in /usr/local/lib/python3.10/dist-packages (from keras_core) (0.0.8)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.10/dist-packages (from keras_core) (3.9.0)\n",
            "Requirement already satisfied: dm-tree in /usr/local/lib/python3.10/dist-packages (from keras_core) (0.1.8)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich->keras_core) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich->keras_core) (2.16.1)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich->keras_core) (0.1.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 60,
      "metadata": {
        "id": "O00JdIwIRZ_U",
        "tags": []
      },
      "outputs": [],
      "source": [
        "import os\n",
        "os.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"3\" # Disable tensorflow debugging logs\n",
        "os.environ[\"KERAS_BACKEND\"] = \"torch\"\n",
        "import keras_core as keras\n",
        "import tensorflow as tf\n",
        "import random\n",
        "import torch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 61,
      "metadata": {
        "tags": [],
        "id": "BfhxWKfjwXMM"
      },
      "outputs": [],
      "source": [
        "import pathlib\n",
        "\n",
        "path_to_zip = tf.keras.utils.get_file(\n",
        "    \"spa-eng.zip\", origin=\"http://storage.googleapis.com/download.tensorflow.org/data/spa-eng.zip\",\n",
        "    extract=True)\n",
        "path_to_file = pathlib.Path(path_to_zip).parent/\"spa-eng/spa.txt\"\n",
        "\n",
        "with open(path_to_file) as f:\n",
        "    lines = f.read().split(\"\\n\")[:-1]\n",
        "\n",
        "text_pairs = []\n",
        "for line in lines:\n",
        "    eng, spa = line.lower().split(\"\\t\")\n",
        "    text_pairs.append((eng, spa))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 62,
      "metadata": {
        "tags": [],
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JiLAXa9twXMN",
        "outputId": "61145b2a-43ae-4950-c717-82f2b0a20f8e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "118964 total pairs\n",
            "118370 training pairs\n",
            "594 validation pairs\n"
          ]
        }
      ],
      "source": [
        "random.Random(43).shuffle(text_pairs)\n",
        "num_val_samples = int(0.005 * len(text_pairs))\n",
        "num_train_samples = len(text_pairs) - num_val_samples\n",
        "train_pairs = text_pairs[:num_train_samples]\n",
        "val_pairs = text_pairs[num_train_samples:]\n",
        "\n",
        "print(f\"{len(text_pairs)} total pairs\")\n",
        "print(f\"{len(train_pairs)} training pairs\")\n",
        "print(f\"{len(val_pairs)} validation pairs\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 63,
      "metadata": {
        "tags": [],
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TlcqI07mwXMN",
        "outputId": "33249a42-bf20-4d8a-add1-e1017ab766a0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "('the old woman fell and could not get up.', 'la anciana se cayó y no pudo levantarse.')\n",
            "('what is this the abbreviation for?', '¿de qué es abreviatura esto?')\n",
            "(\"you're not sick.\", 'no estás enferma.')\n",
            "('i have no knife to cut with.', 'no tengo un cuchillo con que cortarlo.')\n",
            "('americans admire lincoln for his honesty.', 'los estadounidenses admiran a lincoln por su honestidad.')\n"
          ]
        }
      ],
      "source": [
        "for s in train_pairs[:5]:\n",
        "    print(s)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# PIPELINE\n"
      ],
      "metadata": {
        "id": "gzxOciQbeeWy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torchtext.data.utils import get_tokenizer\n",
        "from torchtext.vocab import vocab as Vocab\n",
        "from collections import Counter"
      ],
      "metadata": {
        "id": "k74yIcL9w2c3"
      },
      "execution_count": 64,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!python -m spacy download en_core_web_sm\n",
        "!python -m spacy download es_core_news_sm"
      ],
      "metadata": {
        "id": "qo7PAIyE9RyR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e19c7c26-b2fd-4e01-8067-7ee332845928"
      },
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting en-core-web-sm==3.7.1\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.7.1/en_core_web_sm-3.7.1-py3-none-any.whl (12.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.8/12.8 MB\u001b[0m \u001b[31m31.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: spacy<3.8.0,>=3.7.2 in /usr/local/lib/python3.10/dist-packages (from en-core-web-sm==3.7.1) (3.7.4)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.0.10)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.0.8)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.0.9)\n",
            "Requirement already satisfied: thinc<8.3.0,>=8.2.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (8.2.3)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.1.2)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.4.8)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.0.10)\n",
            "Requirement already satisfied: weasel<0.4.0,>=0.1.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.3.4)\n",
            "Requirement already satisfied: typer<0.10.0,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.9.4)\n",
            "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (6.4.0)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (4.66.2)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.31.0)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.7.0)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.1.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (67.7.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (24.0)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.3.0)\n",
            "Requirement already satisfied: numpy>=1.19.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.25.2)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.6.0)\n",
            "Requirement already satisfied: pydantic-core==2.18.1 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.18.1)\n",
            "Requirement already satisfied: typing-extensions>=4.6.1 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (4.11.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2024.2.2)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.10/dist-packages (from thinc<8.3.0,>=8.2.2->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.7.11)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from thinc<8.3.0,>=8.2.2->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.1.4)\n",
            "Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.10/dist-packages (from typer<0.10.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (8.1.7)\n",
            "Requirement already satisfied: cloudpathlib<0.17.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from weasel<0.4.0,>=0.1.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.16.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.1.5)\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('en_core_web_sm')\n",
            "\u001b[38;5;3m⚠ Restart to reload dependencies\u001b[0m\n",
            "If you are in a Jupyter or Colab notebook, you may need to restart Python in\n",
            "order to load all the package's dependencies. You can do this by selecting the\n",
            "'Restart kernel' or 'Restart runtime' option.\n",
            "Collecting es-core-news-sm==3.7.0\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/es_core_news_sm-3.7.0/es_core_news_sm-3.7.0-py3-none-any.whl (12.9 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.9/12.9 MB\u001b[0m \u001b[31m39.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: spacy<3.8.0,>=3.7.0 in /usr/local/lib/python3.10/dist-packages (from es-core-news-sm==3.7.0) (3.7.4)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (1.0.10)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (2.0.8)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (3.0.9)\n",
            "Requirement already satisfied: thinc<8.3.0,>=8.2.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (8.2.3)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (1.1.2)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (2.4.8)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (2.0.10)\n",
            "Requirement already satisfied: weasel<0.4.0,>=0.1.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (0.3.4)\n",
            "Requirement already satisfied: typer<0.10.0,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (0.9.4)\n",
            "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (6.4.0)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (4.66.2)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (2.31.0)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (2.7.0)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (3.1.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (67.7.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (24.0)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (3.3.0)\n",
            "Requirement already satisfied: numpy>=1.19.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (1.25.2)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (0.6.0)\n",
            "Requirement already satisfied: pydantic-core==2.18.1 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (2.18.1)\n",
            "Requirement already satisfied: typing-extensions>=4.6.1 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (4.11.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (2024.2.2)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.10/dist-packages (from thinc<8.3.0,>=8.2.2->spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (0.7.11)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from thinc<8.3.0,>=8.2.2->spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (0.1.4)\n",
            "Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.10/dist-packages (from typer<0.10.0,>=0.3.0->spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (8.1.7)\n",
            "Requirement already satisfied: cloudpathlib<0.17.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from weasel<0.4.0,>=0.1.0->spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (0.16.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (2.1.5)\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('es_core_news_sm')\n",
            "\u001b[38;5;3m⚠ Restart to reload dependencies\u001b[0m\n",
            "If you are in a Jupyter or Colab notebook, you may need to restart Python in\n",
            "order to load all the package's dependencies. You can do this by selecting the\n",
            "'Restart kernel' or 'Restart runtime' option.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "eng_tokenizer = get_tokenizer(\"spacy\", language=\"en_core_web_sm\")\n",
        "spa_tokenizer = get_tokenizer(\"spacy\", language=\"es_core_news_sm\")"
      ],
      "metadata": {
        "id": "XkaifFNCw7Pu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "40ee6348-9437-4000-be0b-5e56ab010577"
      },
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/spacy/util.py:1740: UserWarning: [W111] Jupyter notebook detected: if using `prefer_gpu()` or `require_gpu()`, include it in the same cell right before `spacy.load()` to ensure that the model is loaded on the correct device. More information: http://spacy.io/usage/v3#jupyter-notebook-gpu\n",
            "  warnings.warn(Warnings.W111)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def build_vocab(text, tokenizers, min_freq=5):\n",
        "    eng_tokenizer, spa_tokenizer = tokenizers\n",
        "    eng_counter = Counter()\n",
        "    spa_counter = Counter()\n",
        "    for eng_string_, spa_string_ in text:\n",
        "        eng_counter.update(eng_tokenizer(eng_string_))\n",
        "        spa_counter.update(spa_tokenizer(spa_string_))\n",
        "    eng_vocab = Vocab(eng_counter, min_freq=min_freq,\n",
        "                       specials=[\"<unk>\", \"<pad>\"])\n",
        "    spa_vocab = Vocab(spa_counter, min_freq=min_freq,\n",
        "                       specials=[\"<unk>\", \"<pad>\", \"<bos>\", \"<eos>\"])\n",
        "    return eng_vocab, spa_vocab\n",
        "\n",
        "eng_vocab, spa_vocab = build_vocab(text_pairs,\n",
        "                                   [eng_tokenizer, spa_tokenizer],\n",
        "                                   min_freq=0)"
      ],
      "metadata": {
        "id": "w1UVOFChzQxI"
      },
      "execution_count": 68,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "eng_vocab_size = len(eng_vocab)\n",
        "spa_vocab_size = len(spa_vocab)\n",
        "eng_vocab_size, spa_vocab_size"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aVxbaNuOz5zC",
        "outputId": "0be63e8b-cdf6-43e9-b75e-db04c31338e2"
      },
      "execution_count": 69,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(13229, 26116)"
            ]
          },
          "metadata": {},
          "execution_count": 69
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "maxlen = 20\n",
        "\n",
        "def data_process(text, eng_vocab, spa_vocab, eng_tokenizer, spa_tokenizer):\n",
        "    data = []\n",
        "    for eng, spa in text:\n",
        "        eng_tensor_ = torch.tensor([eng_vocab[token] for token in eng_tokenizer(eng)],\n",
        "                                dtype=torch.long)\n",
        "        spa_tensor_ = torch.tensor([spa_vocab[token] for token in spa_tokenizer(spa)],\n",
        "                                dtype=torch.long)\n",
        "\n",
        "        if eng_tensor_.shape[0] < maxlen and spa_tensor_.shape[0] < maxlen - 2:\n",
        "            data.append((eng_tensor_, spa_tensor_))\n",
        "    return data\n",
        "\n",
        "train_data = data_process(train_pairs, eng_vocab, spa_vocab, eng_tokenizer, spa_tokenizer)\n",
        "val_data = data_process(val_pairs, eng_vocab, spa_vocab, eng_tokenizer, spa_tokenizer)\n",
        "\n",
        "print(len(train_data), len(val_data))"
      ],
      "metadata": {
        "id": "KHnx-TxW5suR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1abdc289-52d8-4df2-99a7-004c9757c261"
      },
      "execution_count": 70,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "117552 591\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 64\n",
        "PAD_IDX = eng_vocab[\"<pad>\"]\n",
        "BOS_IDX = spa_vocab[\"<bos>\"]\n",
        "EOS_IDX = spa_vocab[\"<eos>\"]\n",
        "UNK_IDX = spa_vocab[\"<unk>\"]\n",
        "\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.nn import functional as F\n",
        "\n",
        "\n",
        "def pad_to_max_length(tensor, max_length):\n",
        "    current_length = tensor.size(1)\n",
        "    if current_length < max_length:\n",
        "        padding_size = max_length - current_length\n",
        "        padding = torch.full((tensor.size(0), padding_size), PAD_IDX, dtype=torch.long)\n",
        "        tensor = torch.cat([tensor, padding], dim=1)\n",
        "    return tensor\n",
        "\n",
        "def generate_batch(data_batch):\n",
        "    x, y = [], []\n",
        "\n",
        "    for (x_item, y_item) in data_batch:\n",
        "\n",
        "        x.append(torch.cat([x_item]))\n",
        "        y.append(torch.cat([torch.tensor([BOS_IDX]),\n",
        "                            y_item,\n",
        "                            torch.tensor([EOS_IDX])], dim=0))\n",
        "\n",
        "    x = pad_sequence(x, batch_first=True, padding_value=PAD_IDX)\n",
        "    y = pad_sequence(y, batch_first=True, padding_value=PAD_IDX)\n",
        "\n",
        "    # Ajustar a la longitud máxima\n",
        "    x = pad_to_max_length(x, maxlen +2)\n",
        "    y = pad_to_max_length(y, maxlen +2)\n",
        "\n",
        "    return x, y ## Aquí tengo mis dudas si\n"
      ],
      "metadata": {
        "id": "HtirYaUR7rEq"
      },
      "execution_count": 71,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_loader = DataLoader(train_data, batch_size=batch_size,\n",
        "                          shuffle=True, collate_fn=generate_batch,\n",
        "                          num_workers=4, pin_memory=True)\n",
        "val_loader = DataLoader(val_data, batch_size=batch_size,\n",
        "                          shuffle=True, collate_fn=generate_batch,\n",
        "                          num_workers=4, pin_memory=True)\n",
        "\n"
      ],
      "metadata": {
        "id": "brERdsjtf1tm",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e5cfa35c-f7a4-4652-b28b-b6b6e84e8183"
      },
      "execution_count": 72,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:558: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        " train_batch, train_target_batch = next(iter(train_loader))\n",
        " train_batch.shape, train_target_batch.shape\n",
        "\n",
        " print(train_target_batch)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5sqU1qvY8fqc",
        "outputId": "e4d9a37a-f157-4a53-b57d-1b8cf654240c"
      },
      "execution_count": 73,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n",
            "  self.pid = os.fork()\n",
            "/usr/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n",
            "  self.pid = os.fork()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[   2,  104,  377,  ...,    1,    1,    1],\n",
            "        [   2,   80,   16,  ...,    1,    1,    1],\n",
            "        [   2,   16,   44,  ...,    1,    1,    1],\n",
            "        ...,\n",
            "        [   2,   13,   91,  ...,    1,    1,    1],\n",
            "        [   2,   31,    4,  ...,    1,    1,    1],\n",
            "        [   2,  344, 2188,  ...,    1,    1,    1]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ATENCION"
      ],
      "metadata": {
        "id": "O2xlD_MEg4nD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "from torch import optim\n",
        "import time"
      ],
      "metadata": {
        "id": "w7ZM-KEXAnMg"
      },
      "execution_count": 74,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Attention(nn.Module): ##SELF ATTENTION\n",
        "    def __init__(self, dim, maxlen, n_heads=4, bias=True):\n",
        "        super().__init__()\n",
        "        self.n_heads = n_heads\n",
        "        self.scale = (dim // n_heads) ** -0.5\n",
        "        self.qw = nn.Linear(dim, dim, bias = bias)\n",
        "        self.kw = nn.Linear(dim, dim, bias = bias)\n",
        "        self.vw = nn.Linear(dim, dim, bias = bias)\n",
        "\n",
        "        self.ow = nn.Linear(dim, dim, bias = bias)\n",
        "        self.register_buffer(\"bias\", torch.tril(torch.ones(maxlen, maxlen)).view(1, 1, maxlen, maxlen))\n",
        "\n",
        "    def forward(self, x):\n",
        "        B, L, D = x.shape\n",
        "        q = self.qw(x)\n",
        "        k = self.kw(x)\n",
        "        v = self.vw(x)\n",
        "\n",
        "        B, L, D = q.shape\n",
        "        q = torch.reshape(q, [B, L, self.n_heads, -1])\n",
        "        q = torch.permute(q, [0, 2, 1, 3])\n",
        "\n",
        "        k = torch.reshape(k, [B, L, self.n_heads, -1])\n",
        "        k = torch.permute(k, [0, 2, 3, 1])\n",
        "        v = torch.reshape(v, [B, L, self.n_heads, -1])\n",
        "        v = torch.permute(v, [0, 2, 1, 3])\n",
        "\n",
        "        qk = torch.matmul(q, k) * self.scale\n",
        "        qk = qk.masked_fill(self.bias[:,:,:L,:L] == 0, float('-inf'))\n",
        "\n",
        "        attn = torch.softmax(qk, dim=-1)\n",
        "\n",
        "        v_attn = torch.matmul(attn, v)\n",
        "        v_attn = torch.permute(v_attn, [0, 2, 1, 3])\n",
        "        v_attn = torch.reshape(v_attn, [B, L, D])\n",
        "\n",
        "        x = self.ow(v_attn)\n",
        "        return x\n",
        "\n",
        "\n",
        "test_layer = Attention(32, maxlen+2, n_heads=1)\n",
        "test_layer(torch.ones([1, maxlen+2, 32]))"
      ],
      "metadata": {
        "id": "2aUpHphMIntX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "37d30061-fd5f-4ca7-bae4-27e9e8e3ed1f"
      },
      "execution_count": 108,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[[-0.2197,  0.4882, -0.5777,  0.1943, -0.4862,  0.1287,  0.3880,\n",
              "           0.1625,  0.0408, -0.2899,  0.5814, -0.3933, -0.4094, -0.3302,\n",
              "          -0.2562,  0.3721, -0.1940, -0.0112, -0.4259, -0.2208, -0.3669,\n",
              "           0.4376, -0.1189,  0.4268,  0.1117,  0.1598,  0.7720,  0.3906,\n",
              "          -0.3718, -0.2117, -0.4396, -0.0301],\n",
              "         [-0.2197,  0.4882, -0.5777,  0.1943, -0.4862,  0.1287,  0.3880,\n",
              "           0.1625,  0.0408, -0.2899,  0.5814, -0.3933, -0.4094, -0.3302,\n",
              "          -0.2562,  0.3721, -0.1940, -0.0112, -0.4259, -0.2208, -0.3669,\n",
              "           0.4376, -0.1189,  0.4268,  0.1117,  0.1598,  0.7720,  0.3906,\n",
              "          -0.3718, -0.2117, -0.4396, -0.0301],\n",
              "         [-0.2197,  0.4882, -0.5777,  0.1943, -0.4862,  0.1287,  0.3880,\n",
              "           0.1625,  0.0408, -0.2899,  0.5814, -0.3933, -0.4094, -0.3302,\n",
              "          -0.2562,  0.3721, -0.1940, -0.0112, -0.4259, -0.2208, -0.3669,\n",
              "           0.4376, -0.1189,  0.4268,  0.1117,  0.1598,  0.7720,  0.3906,\n",
              "          -0.3718, -0.2117, -0.4396, -0.0301],\n",
              "         [-0.2197,  0.4882, -0.5777,  0.1943, -0.4862,  0.1287,  0.3880,\n",
              "           0.1625,  0.0408, -0.2899,  0.5814, -0.3933, -0.4094, -0.3302,\n",
              "          -0.2562,  0.3721, -0.1940, -0.0112, -0.4259, -0.2208, -0.3669,\n",
              "           0.4376, -0.1189,  0.4268,  0.1117,  0.1598,  0.7720,  0.3906,\n",
              "          -0.3718, -0.2117, -0.4396, -0.0301],\n",
              "         [-0.2197,  0.4882, -0.5777,  0.1943, -0.4862,  0.1287,  0.3880,\n",
              "           0.1625,  0.0408, -0.2899,  0.5814, -0.3933, -0.4094, -0.3302,\n",
              "          -0.2562,  0.3721, -0.1940, -0.0112, -0.4259, -0.2208, -0.3669,\n",
              "           0.4376, -0.1189,  0.4268,  0.1117,  0.1598,  0.7720,  0.3906,\n",
              "          -0.3718, -0.2117, -0.4396, -0.0301],\n",
              "         [-0.2197,  0.4882, -0.5777,  0.1943, -0.4862,  0.1287,  0.3880,\n",
              "           0.1625,  0.0408, -0.2899,  0.5814, -0.3933, -0.4094, -0.3302,\n",
              "          -0.2562,  0.3721, -0.1940, -0.0112, -0.4259, -0.2208, -0.3669,\n",
              "           0.4376, -0.1189,  0.4268,  0.1117,  0.1598,  0.7720,  0.3906,\n",
              "          -0.3718, -0.2117, -0.4396, -0.0301],\n",
              "         [-0.2197,  0.4882, -0.5777,  0.1943, -0.4862,  0.1287,  0.3880,\n",
              "           0.1625,  0.0408, -0.2899,  0.5814, -0.3933, -0.4094, -0.3302,\n",
              "          -0.2562,  0.3721, -0.1940, -0.0112, -0.4259, -0.2208, -0.3669,\n",
              "           0.4376, -0.1189,  0.4268,  0.1117,  0.1598,  0.7720,  0.3906,\n",
              "          -0.3718, -0.2117, -0.4396, -0.0301],\n",
              "         [-0.2197,  0.4882, -0.5777,  0.1943, -0.4862,  0.1287,  0.3880,\n",
              "           0.1625,  0.0408, -0.2899,  0.5814, -0.3933, -0.4094, -0.3302,\n",
              "          -0.2562,  0.3721, -0.1940, -0.0112, -0.4259, -0.2208, -0.3669,\n",
              "           0.4376, -0.1189,  0.4268,  0.1117,  0.1598,  0.7720,  0.3906,\n",
              "          -0.3718, -0.2117, -0.4396, -0.0301],\n",
              "         [-0.2197,  0.4882, -0.5777,  0.1943, -0.4862,  0.1287,  0.3880,\n",
              "           0.1625,  0.0408, -0.2899,  0.5814, -0.3933, -0.4094, -0.3302,\n",
              "          -0.2562,  0.3721, -0.1940, -0.0112, -0.4259, -0.2208, -0.3669,\n",
              "           0.4376, -0.1189,  0.4268,  0.1117,  0.1598,  0.7720,  0.3906,\n",
              "          -0.3718, -0.2117, -0.4396, -0.0301],\n",
              "         [-0.2197,  0.4882, -0.5777,  0.1943, -0.4862,  0.1287,  0.3880,\n",
              "           0.1625,  0.0408, -0.2899,  0.5814, -0.3933, -0.4094, -0.3302,\n",
              "          -0.2562,  0.3721, -0.1940, -0.0112, -0.4259, -0.2208, -0.3669,\n",
              "           0.4376, -0.1189,  0.4268,  0.1117,  0.1598,  0.7720,  0.3906,\n",
              "          -0.3718, -0.2117, -0.4396, -0.0301],\n",
              "         [-0.2197,  0.4882, -0.5777,  0.1943, -0.4862,  0.1287,  0.3880,\n",
              "           0.1625,  0.0408, -0.2899,  0.5814, -0.3933, -0.4094, -0.3302,\n",
              "          -0.2562,  0.3721, -0.1940, -0.0112, -0.4259, -0.2208, -0.3669,\n",
              "           0.4376, -0.1189,  0.4268,  0.1117,  0.1598,  0.7720,  0.3906,\n",
              "          -0.3718, -0.2117, -0.4396, -0.0301],\n",
              "         [-0.2197,  0.4882, -0.5777,  0.1943, -0.4862,  0.1287,  0.3880,\n",
              "           0.1625,  0.0408, -0.2899,  0.5814, -0.3933, -0.4094, -0.3302,\n",
              "          -0.2562,  0.3721, -0.1940, -0.0112, -0.4259, -0.2208, -0.3669,\n",
              "           0.4376, -0.1189,  0.4268,  0.1117,  0.1598,  0.7720,  0.3906,\n",
              "          -0.3718, -0.2117, -0.4396, -0.0301],\n",
              "         [-0.2197,  0.4882, -0.5777,  0.1943, -0.4862,  0.1287,  0.3880,\n",
              "           0.1625,  0.0408, -0.2899,  0.5814, -0.3933, -0.4094, -0.3302,\n",
              "          -0.2562,  0.3721, -0.1940, -0.0112, -0.4259, -0.2208, -0.3669,\n",
              "           0.4376, -0.1189,  0.4268,  0.1117,  0.1598,  0.7720,  0.3906,\n",
              "          -0.3718, -0.2117, -0.4396, -0.0301],\n",
              "         [-0.2197,  0.4882, -0.5777,  0.1943, -0.4862,  0.1287,  0.3880,\n",
              "           0.1625,  0.0408, -0.2899,  0.5814, -0.3933, -0.4094, -0.3302,\n",
              "          -0.2562,  0.3721, -0.1940, -0.0112, -0.4259, -0.2208, -0.3669,\n",
              "           0.4376, -0.1189,  0.4268,  0.1117,  0.1598,  0.7720,  0.3906,\n",
              "          -0.3718, -0.2117, -0.4396, -0.0301],\n",
              "         [-0.2197,  0.4882, -0.5777,  0.1943, -0.4862,  0.1287,  0.3880,\n",
              "           0.1625,  0.0408, -0.2899,  0.5814, -0.3933, -0.4094, -0.3302,\n",
              "          -0.2562,  0.3721, -0.1940, -0.0112, -0.4259, -0.2208, -0.3669,\n",
              "           0.4376, -0.1189,  0.4268,  0.1117,  0.1598,  0.7720,  0.3906,\n",
              "          -0.3718, -0.2117, -0.4396, -0.0301],\n",
              "         [-0.2197,  0.4882, -0.5777,  0.1943, -0.4862,  0.1287,  0.3880,\n",
              "           0.1625,  0.0408, -0.2899,  0.5814, -0.3933, -0.4094, -0.3302,\n",
              "          -0.2562,  0.3721, -0.1940, -0.0112, -0.4259, -0.2208, -0.3669,\n",
              "           0.4376, -0.1189,  0.4268,  0.1117,  0.1598,  0.7720,  0.3906,\n",
              "          -0.3718, -0.2117, -0.4396, -0.0301],\n",
              "         [-0.2197,  0.4882, -0.5777,  0.1943, -0.4862,  0.1287,  0.3880,\n",
              "           0.1625,  0.0408, -0.2899,  0.5814, -0.3933, -0.4094, -0.3302,\n",
              "          -0.2562,  0.3721, -0.1940, -0.0112, -0.4259, -0.2208, -0.3669,\n",
              "           0.4376, -0.1189,  0.4268,  0.1117,  0.1598,  0.7720,  0.3906,\n",
              "          -0.3718, -0.2117, -0.4396, -0.0301],\n",
              "         [-0.2197,  0.4882, -0.5777,  0.1943, -0.4862,  0.1287,  0.3880,\n",
              "           0.1625,  0.0408, -0.2899,  0.5814, -0.3933, -0.4094, -0.3302,\n",
              "          -0.2562,  0.3721, -0.1940, -0.0112, -0.4259, -0.2208, -0.3669,\n",
              "           0.4376, -0.1189,  0.4268,  0.1117,  0.1598,  0.7720,  0.3906,\n",
              "          -0.3718, -0.2117, -0.4396, -0.0301],\n",
              "         [-0.2197,  0.4882, -0.5777,  0.1943, -0.4862,  0.1287,  0.3880,\n",
              "           0.1625,  0.0408, -0.2899,  0.5814, -0.3933, -0.4094, -0.3302,\n",
              "          -0.2562,  0.3721, -0.1940, -0.0112, -0.4259, -0.2208, -0.3669,\n",
              "           0.4376, -0.1189,  0.4268,  0.1117,  0.1598,  0.7720,  0.3906,\n",
              "          -0.3718, -0.2117, -0.4396, -0.0301],\n",
              "         [-0.2197,  0.4882, -0.5777,  0.1943, -0.4862,  0.1287,  0.3880,\n",
              "           0.1625,  0.0408, -0.2899,  0.5814, -0.3933, -0.4094, -0.3302,\n",
              "          -0.2562,  0.3721, -0.1940, -0.0112, -0.4259, -0.2208, -0.3669,\n",
              "           0.4376, -0.1189,  0.4268,  0.1117,  0.1598,  0.7720,  0.3906,\n",
              "          -0.3718, -0.2117, -0.4396, -0.0301],\n",
              "         [-0.2197,  0.4882, -0.5777,  0.1943, -0.4862,  0.1287,  0.3880,\n",
              "           0.1625,  0.0408, -0.2899,  0.5814, -0.3933, -0.4094, -0.3302,\n",
              "          -0.2562,  0.3721, -0.1940, -0.0112, -0.4259, -0.2208, -0.3669,\n",
              "           0.4376, -0.1189,  0.4268,  0.1117,  0.1598,  0.7720,  0.3906,\n",
              "          -0.3718, -0.2117, -0.4396, -0.0301],\n",
              "         [-0.2197,  0.4882, -0.5777,  0.1943, -0.4862,  0.1287,  0.3880,\n",
              "           0.1625,  0.0408, -0.2899,  0.5814, -0.3933, -0.4094, -0.3302,\n",
              "          -0.2562,  0.3721, -0.1940, -0.0112, -0.4259, -0.2208, -0.3669,\n",
              "           0.4376, -0.1189,  0.4268,  0.1117,  0.1598,  0.7720,  0.3906,\n",
              "          -0.3718, -0.2117, -0.4396, -0.0301]]], grad_fn=<ViewBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 108
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "class CrossAttention(nn.Module): ##CROSS ATTENTION\n",
        "    def __init__(self, dim, maxlen, n_heads=4, bias=True):\n",
        "        super().__init__()\n",
        "        self.n_heads = n_heads\n",
        "        self.scale = (dim // n_heads) ** -0.5\n",
        "        self.qw = nn.Linear(dim, dim, bias=bias)\n",
        "        self.kw = nn.Linear(dim, dim, bias=bias)\n",
        "        self.vw = nn.Linear(dim, dim, bias=bias)\n",
        "\n",
        "        self.ow = nn.Linear(dim, dim, bias=bias)\n",
        "        self.register_buffer(\"bias\", torch.tril(torch.ones(maxlen, maxlen)).view(1, 1, maxlen, maxlen))\n",
        "\n",
        "    def forward(self, x): # k y v son la frase en inglés. Contexto\n",
        "\n",
        "        B, L, D = x.shape\n",
        "        q = self.qw(x)\n",
        "        k = self.kw(x)\n",
        "        v = self.vw(x)\n",
        "\n",
        "        B, L_context, D = k.shape\n",
        "        q = torch.reshape(q, [B, L, self.n_heads, -1])\n",
        "        q = torch.permute(q, [0, 2, 1, 3])\n",
        "        k = torch.reshape(k, [B, L_context, self.n_heads, -1])\n",
        "        k = torch.permute(k, [0, 2, 3, 1])\n",
        "        v = torch.reshape(v, [B, L_context, self.n_heads, -1])\n",
        "        v = torch.permute(v, [0, 2, 1, 3])\n",
        "\n",
        "        qk = torch.matmul(q, k) * self.scale\n",
        "        qk = qk.masked_fill(self.bias[:, :, :L, :L_context] == 0, float('-inf'))\n",
        "\n",
        "        attn = torch.softmax(qk, dim=-1)\n",
        "\n",
        "        v_attn = torch.matmul(attn, v)\n",
        "        v_attn = torch.permute(v_attn, [0, 2, 1, 3])\n",
        "        v_attn = torch.reshape(v_attn, [B, L, D])\n",
        "\n",
        "        x = self.ow(v_attn)\n",
        "        return x\n",
        "\n",
        "test_layer = CrossAttention(32, maxlen + 2, n_heads=1)\n",
        "inputs = torch.ones([1, maxlen, 32])\n",
        "#context = torch.ones([ maxlen-6, 32])\n",
        "#context2 = torch.ones([ maxlen-6, 32])\n",
        "test_layer(inputs)\n"
      ],
      "metadata": {
        "id": "U4M-Ccj8iZie",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "636b44bd-58c9-4438-9c8a-dd39c1a2f5ad"
      },
      "execution_count": 109,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[[-0.1370, -0.2503, -0.1604,  0.1016,  0.1489, -0.3069,  0.0478,\n",
              "          -0.0468, -0.7308,  0.4106, -0.0410,  0.3223, -0.2518,  0.1145,\n",
              "           0.2870,  0.0367, -0.3215, -0.1188,  0.0482,  0.0102, -0.0298,\n",
              "           0.1470, -0.0341, -0.3269,  0.2657,  0.3793, -0.0728,  0.0913,\n",
              "          -0.0336, -0.0882, -0.1693,  0.1536],\n",
              "         [-0.1370, -0.2503, -0.1604,  0.1016,  0.1489, -0.3069,  0.0478,\n",
              "          -0.0468, -0.7308,  0.4106, -0.0410,  0.3223, -0.2518,  0.1145,\n",
              "           0.2870,  0.0367, -0.3215, -0.1188,  0.0482,  0.0102, -0.0298,\n",
              "           0.1470, -0.0341, -0.3269,  0.2657,  0.3793, -0.0728,  0.0913,\n",
              "          -0.0336, -0.0882, -0.1693,  0.1536],\n",
              "         [-0.1370, -0.2503, -0.1604,  0.1016,  0.1489, -0.3069,  0.0478,\n",
              "          -0.0468, -0.7308,  0.4106, -0.0410,  0.3223, -0.2518,  0.1145,\n",
              "           0.2870,  0.0367, -0.3215, -0.1188,  0.0482,  0.0102, -0.0298,\n",
              "           0.1470, -0.0341, -0.3269,  0.2657,  0.3793, -0.0728,  0.0913,\n",
              "          -0.0336, -0.0882, -0.1693,  0.1536],\n",
              "         [-0.1370, -0.2503, -0.1604,  0.1016,  0.1489, -0.3069,  0.0478,\n",
              "          -0.0468, -0.7308,  0.4106, -0.0410,  0.3223, -0.2518,  0.1145,\n",
              "           0.2870,  0.0367, -0.3215, -0.1188,  0.0482,  0.0102, -0.0298,\n",
              "           0.1470, -0.0341, -0.3269,  0.2657,  0.3793, -0.0728,  0.0913,\n",
              "          -0.0336, -0.0882, -0.1693,  0.1536],\n",
              "         [-0.1370, -0.2503, -0.1604,  0.1016,  0.1489, -0.3069,  0.0478,\n",
              "          -0.0468, -0.7308,  0.4106, -0.0410,  0.3223, -0.2518,  0.1145,\n",
              "           0.2870,  0.0367, -0.3215, -0.1188,  0.0482,  0.0102, -0.0298,\n",
              "           0.1470, -0.0341, -0.3269,  0.2657,  0.3793, -0.0728,  0.0913,\n",
              "          -0.0336, -0.0882, -0.1693,  0.1536],\n",
              "         [-0.1370, -0.2503, -0.1604,  0.1016,  0.1489, -0.3069,  0.0478,\n",
              "          -0.0468, -0.7308,  0.4106, -0.0410,  0.3223, -0.2518,  0.1145,\n",
              "           0.2870,  0.0367, -0.3215, -0.1188,  0.0482,  0.0102, -0.0298,\n",
              "           0.1470, -0.0341, -0.3269,  0.2657,  0.3793, -0.0728,  0.0913,\n",
              "          -0.0336, -0.0882, -0.1693,  0.1536],\n",
              "         [-0.1370, -0.2503, -0.1604,  0.1016,  0.1489, -0.3069,  0.0478,\n",
              "          -0.0468, -0.7308,  0.4106, -0.0410,  0.3223, -0.2518,  0.1145,\n",
              "           0.2870,  0.0367, -0.3215, -0.1188,  0.0482,  0.0102, -0.0298,\n",
              "           0.1470, -0.0341, -0.3269,  0.2657,  0.3793, -0.0728,  0.0913,\n",
              "          -0.0336, -0.0882, -0.1693,  0.1536],\n",
              "         [-0.1370, -0.2503, -0.1604,  0.1016,  0.1489, -0.3069,  0.0478,\n",
              "          -0.0468, -0.7308,  0.4106, -0.0410,  0.3223, -0.2518,  0.1145,\n",
              "           0.2870,  0.0367, -0.3215, -0.1188,  0.0482,  0.0102, -0.0298,\n",
              "           0.1470, -0.0341, -0.3269,  0.2657,  0.3793, -0.0728,  0.0913,\n",
              "          -0.0336, -0.0882, -0.1693,  0.1536],\n",
              "         [-0.1370, -0.2503, -0.1604,  0.1016,  0.1489, -0.3069,  0.0478,\n",
              "          -0.0468, -0.7308,  0.4106, -0.0410,  0.3223, -0.2518,  0.1145,\n",
              "           0.2870,  0.0367, -0.3215, -0.1188,  0.0482,  0.0102, -0.0298,\n",
              "           0.1470, -0.0341, -0.3269,  0.2657,  0.3793, -0.0728,  0.0913,\n",
              "          -0.0336, -0.0882, -0.1693,  0.1536],\n",
              "         [-0.1370, -0.2503, -0.1604,  0.1016,  0.1489, -0.3069,  0.0478,\n",
              "          -0.0468, -0.7308,  0.4106, -0.0410,  0.3223, -0.2518,  0.1145,\n",
              "           0.2870,  0.0367, -0.3215, -0.1188,  0.0482,  0.0102, -0.0298,\n",
              "           0.1470, -0.0341, -0.3269,  0.2657,  0.3793, -0.0728,  0.0913,\n",
              "          -0.0336, -0.0882, -0.1693,  0.1536],\n",
              "         [-0.1370, -0.2503, -0.1604,  0.1016,  0.1489, -0.3069,  0.0478,\n",
              "          -0.0468, -0.7308,  0.4106, -0.0410,  0.3223, -0.2518,  0.1145,\n",
              "           0.2870,  0.0367, -0.3215, -0.1188,  0.0482,  0.0102, -0.0298,\n",
              "           0.1470, -0.0341, -0.3269,  0.2657,  0.3793, -0.0728,  0.0913,\n",
              "          -0.0336, -0.0882, -0.1693,  0.1536],\n",
              "         [-0.1370, -0.2503, -0.1604,  0.1016,  0.1489, -0.3069,  0.0478,\n",
              "          -0.0468, -0.7308,  0.4106, -0.0410,  0.3223, -0.2518,  0.1145,\n",
              "           0.2870,  0.0367, -0.3215, -0.1188,  0.0482,  0.0102, -0.0298,\n",
              "           0.1470, -0.0341, -0.3269,  0.2657,  0.3793, -0.0728,  0.0913,\n",
              "          -0.0336, -0.0882, -0.1693,  0.1536],\n",
              "         [-0.1370, -0.2503, -0.1604,  0.1016,  0.1489, -0.3069,  0.0478,\n",
              "          -0.0468, -0.7308,  0.4106, -0.0410,  0.3223, -0.2518,  0.1145,\n",
              "           0.2870,  0.0367, -0.3215, -0.1188,  0.0482,  0.0102, -0.0298,\n",
              "           0.1470, -0.0341, -0.3269,  0.2657,  0.3793, -0.0728,  0.0913,\n",
              "          -0.0336, -0.0882, -0.1693,  0.1536],\n",
              "         [-0.1370, -0.2503, -0.1604,  0.1016,  0.1489, -0.3069,  0.0478,\n",
              "          -0.0468, -0.7308,  0.4106, -0.0410,  0.3223, -0.2518,  0.1145,\n",
              "           0.2870,  0.0367, -0.3215, -0.1188,  0.0482,  0.0102, -0.0298,\n",
              "           0.1470, -0.0341, -0.3269,  0.2657,  0.3793, -0.0728,  0.0913,\n",
              "          -0.0336, -0.0882, -0.1693,  0.1536],\n",
              "         [-0.1370, -0.2503, -0.1604,  0.1016,  0.1489, -0.3069,  0.0478,\n",
              "          -0.0468, -0.7308,  0.4106, -0.0410,  0.3223, -0.2518,  0.1145,\n",
              "           0.2870,  0.0367, -0.3215, -0.1188,  0.0482,  0.0102, -0.0298,\n",
              "           0.1470, -0.0341, -0.3269,  0.2657,  0.3793, -0.0728,  0.0913,\n",
              "          -0.0336, -0.0882, -0.1693,  0.1536],\n",
              "         [-0.1370, -0.2503, -0.1604,  0.1016,  0.1489, -0.3069,  0.0478,\n",
              "          -0.0468, -0.7308,  0.4106, -0.0410,  0.3223, -0.2518,  0.1145,\n",
              "           0.2870,  0.0367, -0.3215, -0.1188,  0.0482,  0.0102, -0.0298,\n",
              "           0.1470, -0.0341, -0.3269,  0.2657,  0.3793, -0.0728,  0.0913,\n",
              "          -0.0336, -0.0882, -0.1693,  0.1536],\n",
              "         [-0.1370, -0.2503, -0.1604,  0.1016,  0.1489, -0.3069,  0.0478,\n",
              "          -0.0468, -0.7308,  0.4106, -0.0410,  0.3223, -0.2518,  0.1145,\n",
              "           0.2870,  0.0367, -0.3215, -0.1188,  0.0482,  0.0102, -0.0298,\n",
              "           0.1470, -0.0341, -0.3269,  0.2657,  0.3793, -0.0728,  0.0913,\n",
              "          -0.0336, -0.0882, -0.1693,  0.1536],\n",
              "         [-0.1370, -0.2503, -0.1604,  0.1016,  0.1489, -0.3069,  0.0478,\n",
              "          -0.0468, -0.7308,  0.4106, -0.0410,  0.3223, -0.2518,  0.1145,\n",
              "           0.2870,  0.0367, -0.3215, -0.1188,  0.0482,  0.0102, -0.0298,\n",
              "           0.1470, -0.0341, -0.3269,  0.2657,  0.3793, -0.0728,  0.0913,\n",
              "          -0.0336, -0.0882, -0.1693,  0.1536],\n",
              "         [-0.1370, -0.2503, -0.1604,  0.1016,  0.1489, -0.3069,  0.0478,\n",
              "          -0.0468, -0.7308,  0.4106, -0.0410,  0.3223, -0.2518,  0.1145,\n",
              "           0.2870,  0.0367, -0.3215, -0.1188,  0.0482,  0.0102, -0.0298,\n",
              "           0.1470, -0.0341, -0.3269,  0.2657,  0.3793, -0.0728,  0.0913,\n",
              "          -0.0336, -0.0882, -0.1693,  0.1536],\n",
              "         [-0.1370, -0.2503, -0.1604,  0.1016,  0.1489, -0.3069,  0.0478,\n",
              "          -0.0468, -0.7308,  0.4106, -0.0410,  0.3223, -0.2518,  0.1145,\n",
              "           0.2870,  0.0367, -0.3215, -0.1188,  0.0482,  0.0102, -0.0298,\n",
              "           0.1470, -0.0341, -0.3269,  0.2657,  0.3793, -0.0728,  0.0913,\n",
              "          -0.0336, -0.0882, -0.1693,  0.1536]]], grad_fn=<ViewBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 109
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "class CrossAttention3(nn.Module):\n",
        "    def __init__(self, input_dim, context_dim):\n",
        "        super().__init__()\n",
        "        self.q_linear = nn.Linear(input_dim, input_dim)\n",
        "        self.k_linear = nn.Linear(context_dim, input_dim)\n",
        "        self.v_linear = nn.Linear(context_dim, input_dim)\n",
        "        self.out = nn.Linear(input_dim, input_dim)\n",
        "\n",
        "    def forward(self, input_seq, context_seq):\n",
        "        q = self.q_linear(input_seq)\n",
        "        k = self.k_linear(context_seq)\n",
        "        v = self.v_linear(context_seq)\n",
        "\n",
        "        attention_weights = torch.matmul(q, k.transpose(-2, -1))\n",
        "        attention_weights = torch.softmax(attention_weights, dim=-1)\n",
        "\n",
        "        output = torch.matmul(attention_weights, v)\n",
        "        output = self.out(output)\n",
        "\n",
        "        return output\n",
        "\n",
        "# Ejemplo de uso\n",
        "input_dim = 64\n",
        "context_dim = 128\n",
        "max_seq_len = 10\n",
        "\n",
        "cross_attn = CrossAttention3(input_dim, context_dim)\n",
        "input_seq = torch.randn(1, max_seq_len, input_dim)  # Tamaño del lote, longitud de la secuencia, dimensión de entrada\n",
        "context_seq = torch.randn(1, max_seq_len, context_dim)  # Tamaño del lote, longitud de la secuencia, dimensión de contexto\n",
        "\n",
        "output = cross_attn(input_seq, context_seq)\n",
        "print(output.shape)  # Salida: torch.Size([1, 10, 64])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "da3M8CQYI3uJ",
        "outputId": "31a389d3-63a8-4926-8344-f324efb059a5"
      },
      "execution_count": 110,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([1, 10, 64])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "UazVmTxOL9HB"
      },
      "execution_count": 110,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Transformer(nn.Module):\n",
        "    def __init__(self, dim, maxlen, heads=4, mlp_dim=512, rate=0.0):\n",
        "        super().__init__()\n",
        "        #self.ln_1 = nn.LayerNorm(dim)\n",
        "        #self.c_attn = CrossAttention(dim, maxlen)\n",
        "        self.ln_2 = nn.LayerNorm(dim)\n",
        "        self.attn = Attention(dim, maxlen)\n",
        "        self.ln_3 = nn.LayerNorm(dim)\n",
        "        self.mlp = nn.Sequential(\n",
        "            nn.Linear(dim, mlp_dim),\n",
        "            nn.GELU(),\n",
        "            nn.Dropout(rate),\n",
        "            nn.Linear(mlp_dim, dim),\n",
        "            nn.Dropout(rate),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        #x = self.c_attn(self.ln_1(x)) + x\n",
        "        x = self.attn(self.ln_2(x)) + x\n",
        "        return self.mlp(self.ln_3(x)) + x\n",
        "\n",
        "\n",
        "test_layer = Transformer(32, maxlen + 2)\n",
        "output = test_layer( torch.ones([1, maxlen + 2, 32]))\n",
        "print(output.shape)  # Debería imprimir el tamaño del tensor resultant"
      ],
      "metadata": {
        "id": "Pv0_JhpXk0iY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "814a440b-31e3-43da-9743-ab7d960c228e"
      },
      "execution_count": 111,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([1, 22, 32])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_batch.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "064vpsAVkAta",
        "outputId": "7d607e3b-3be1-4aea-8d0d-ffc7ff217e70"
      },
      "execution_count": 112,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([64, 22])"
            ]
          },
          "metadata": {},
          "execution_count": 112
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class GPT(nn.Module):\n",
        "    def __init__(self, dim, vocab_size_spa, vocab_size_eng, maxlen, depth=3,\n",
        "                 mlp_dim=512, rate=0.2):\n",
        "        super().__init__()\n",
        "        self.embedding_spa = nn.Embedding(vocab_size_spa, dim)\n",
        "        self.pos_embedding_spa = nn.Parameter(\n",
        "            torch.randn(1, maxlen, dim))\n",
        "        self.embedding_eng = nn.Embedding(vocab_size_eng, dim)\n",
        "        self.pos_embedding_eng = nn.Parameter(\n",
        "            torch.randn(1, maxlen, dim))\n",
        "\n",
        "        self.transformer = nn.Sequential()\n",
        "        for _ in range(depth):\n",
        "            self.transformer.append(Transformer(dim, maxlen))\n",
        "\n",
        "        self.head = nn.Linear(dim, vocab_size_spa, bias=False)\n",
        "        self.c_attn = CrossAttention3(dim, dim)\n",
        "\n",
        "\n",
        "    def forward(self, x, y):\n",
        "        Bx, Lx = x.shape\n",
        "\n",
        "        print(x.shape)\n",
        "        By, Ly = y.reshape(1, -1).shape\n",
        "\n",
        "        x = self.embedding_spa(x)\n",
        "        x += self.pos_embedding_spa[:, :Lx]\n",
        "        y = self.embedding_spa(y)\n",
        "        y += self.pos_embedding_spa[:, :Ly]\n",
        "\n",
        "        print(x.shape)\n",
        "        x = self.c_attn(x, y)\n",
        "\n",
        "        print(x.shape)\n",
        "        x = self.transformer(x)\n",
        "        x = self.head(x)\n",
        "        #x = self.fc(x)\n",
        "        return x, y\n",
        "\n",
        "\n",
        "model_dim = 22\n",
        "depth = 3\n",
        "mlp_dim = 22\n",
        "\n",
        "gpt = GPT(dim=model_dim, vocab_size_spa=spa_vocab_size, vocab_size_eng = eng_vocab_size,\n",
        "          maxlen=maxlen + 2, depth=depth, mlp_dim=mlp_dim)\n",
        "\n",
        "\n",
        "output, _ = gpt(train_batch, train_target_batch)\n",
        "output.shape, train_target_batch.shape\n",
        "\n"
      ],
      "metadata": {
        "id": "mEbB6pNKmy4s",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 417
        },
        "outputId": "ee990f56-f064-45b0-f484-57615054d98d"
      },
      "execution_count": 114,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([64, 22])\n",
            "torch.Size([64, 22, 22])\n",
            "torch.Size([64, 22, 22])\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "shape '[64, 22, 4, -1]' is invalid for input of size 30976",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-114-0d6d1a7ebb5e>\u001b[0m in \u001b[0;36m<cell line: 49>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 49\u001b[0;31m \u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgpt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_target_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     50\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_target_batch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1510\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1511\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1512\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1513\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1518\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1519\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1521\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1522\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-114-0d6d1a7ebb5e>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x, y)\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransformer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     36\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m         \u001b[0;31m#x = self.fc(x)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1510\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1511\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1512\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1513\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1518\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1519\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1521\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1522\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    215\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    216\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 217\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    218\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    219\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1510\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1511\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1512\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1513\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1518\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1519\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1521\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1522\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-111-51d8405d6c56>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0;31m#x = self.c_attn(self.ln_1(x)) + x\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mattn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mln_2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmlp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mln_3\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1510\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1511\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1512\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1513\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1518\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1519\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1521\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1522\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-108-dbb18b1de469>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m         \u001b[0mB\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mL\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mD\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m         \u001b[0mq\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mB\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mL\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_heads\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m         \u001b[0mq\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpermute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: shape '[64, 22, 4, -1]' is invalid for input of size 30976"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ENTRENAMIENTO"
      ],
      "metadata": {
        "id": "NHZJwXjFkjNE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
        "print(device)\n",
        "\n",
        "gpt.to(device)"
      ],
      "metadata": {
        "id": "I0nM-uUzmsRl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "PAD_IDX = spa_vocab.get_stoi()['<pad>']\n",
        "PAD_IDX"
      ],
      "metadata": {
        "id": "uSRB-kqQ0Z57"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "optimizer = optim.Adam(gpt.parameters(), lr=0.001)\n",
        "loss_fn = torch.nn.CrossEntropyLoss(ignore_index=PAD_IDX)"
      ],
      "metadata": {
        "id": "mAH0OIf00asK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train(model, device, train_loader, optimizer, epoch):\n",
        "    start = time.time()\n",
        "    running_loss = 0.0\n",
        "    model.train()\n",
        "\n",
        "    for inputs, targets in train_loader:\n",
        "\n",
        "        targets = targets.view(-1)\n",
        "        inputs, targets = inputs.to(device), targets.to(device)\n",
        "\n",
        "        # zero the parameter gradients\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # forward + backward + optimize\n",
        "        outputs, _ = model(inputs, targets)\n",
        "        outputs = outputs.view(-1, outputs.size(-1))\n",
        "        loss = loss_fn(outputs, targets)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += loss.item()\n",
        "\n",
        "    print(f'\\nTime for epoch {epoch} is {time.time()-start:4f} sec Train loss: {running_loss / len(train_loader):4f}')"
      ],
      "metadata": {
        "id": "7LpeVUem0dZV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def translate(model, sentence, device, maxlen):\n",
        "    with torch.no_grad():\n",
        "        model.eval()\n",
        "        idx = torch.tensor([eng_vocab[token] for token in eng_tokenizer(sentence)],\n",
        "                                    dtype=torch.long)\n",
        "        idx = idx.reshape([1, -1])\n",
        "        maxlen = maxlen - idx.shape[-1]\n",
        "\n",
        "        for _ in range(maxlen):\n",
        "            idx = idx.to(device)\n",
        "            logits = model(idx, idx)[0][:, -1, :]\n",
        "            probs = torch.softmax(logits, dim=-1)\n",
        "\n",
        "            _, idx_next = torch.topk(probs, k=1, dim=-1)\n",
        "            idx = torch.cat((idx, idx_next), dim=1)\n",
        "\n",
        "        txt = \" \".join(\n",
        "                    [spa_vocab.get_itos()[idx[0, _]] for _ in range(maxlen)]\n",
        "                )\n",
        "    return txt.replace(\"<eos>\", \"\")\n",
        "\n",
        "sentences = [\"he drinks coffee while reading the newspaper headlines\",\n",
        "             \"families gather for dinner and games\"]\n",
        "\n",
        "for s in sentences:\n",
        "    trans = translate(gpt, s, device, maxlen + 2)\n",
        "    print(f\"\\n{trans}\")"
      ],
      "metadata": {
        "id": "uzv0zMlw0u-6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "epochs = 10\n",
        "\n",
        "for epoch in range(epochs):\n",
        "    train(gpt, device, train_loader, optimizer, epoch)\n",
        "\n",
        "    # Translate test sentences\n",
        "    for s in sentences:\n",
        "        trans = translate(gpt, s, device, maxlen + 2)\n",
        "        print(s +\" : \"+trans)"
      ],
      "metadata": {
        "id": "M5obVVPTBI-e"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}