{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Programa 3\n",
        "María Emilia Ramírez Gómez"
      ],
      "metadata": {
        "id": "58Nactwo5tzO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install keras_core"
      ],
      "metadata": {
        "id": "21XfrsMF8vpS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "670092c4-7d6e-45c1-d2cd-db7689545db0"
      },
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: keras_core in /usr/local/lib/python3.10/dist-packages (0.1.7)\n",
            "Requirement already satisfied: absl-py in /usr/local/lib/python3.10/dist-packages (from keras_core) (1.4.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from keras_core) (1.25.2)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.10/dist-packages (from keras_core) (13.7.1)\n",
            "Requirement already satisfied: namex in /usr/local/lib/python3.10/dist-packages (from keras_core) (0.0.8)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.10/dist-packages (from keras_core) (3.9.0)\n",
            "Requirement already satisfied: dm-tree in /usr/local/lib/python3.10/dist-packages (from keras_core) (0.1.8)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich->keras_core) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich->keras_core) (2.16.1)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich->keras_core) (0.1.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 59,
      "metadata": {
        "id": "O00JdIwIRZ_U",
        "tags": []
      },
      "outputs": [],
      "source": [
        "import os\n",
        "os.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"3\" # Disable tensorflow debugging logs\n",
        "os.environ[\"KERAS_BACKEND\"] = \"torch\"\n",
        "import keras_core as keras\n",
        "import tensorflow as tf\n",
        "import random\n",
        "import torch"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qtBxiYCOwXML"
      },
      "source": [
        "## 1.- Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 60,
      "metadata": {
        "tags": [],
        "id": "BfhxWKfjwXMM"
      },
      "outputs": [],
      "source": [
        "import pathlib\n",
        "\n",
        "path_to_zip = tf.keras.utils.get_file(\n",
        "    \"spa-eng.zip\", origin=\"http://storage.googleapis.com/download.tensorflow.org/data/spa-eng.zip\",\n",
        "    extract=True)\n",
        "path_to_file = pathlib.Path(path_to_zip).parent/\"spa-eng/spa.txt\"\n",
        "\n",
        "with open(path_to_file) as f:\n",
        "    lines = f.read().split(\"\\n\")[:-1]\n",
        "\n",
        "text_pairs = []\n",
        "for line in lines:\n",
        "    eng, spa = line.lower().split(\"\\t\")\n",
        "    text_pairs.append((eng, spa))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 61,
      "metadata": {
        "tags": [],
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JiLAXa9twXMN",
        "outputId": "526ebbab-0add-4ae7-e5d2-251e1a6b5886"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "118964 total pairs\n",
            "118370 training pairs\n",
            "594 validation pairs\n"
          ]
        }
      ],
      "source": [
        "random.Random(43).shuffle(text_pairs)\n",
        "num_val_samples = int(0.005 * len(text_pairs))\n",
        "num_train_samples = len(text_pairs) - num_val_samples\n",
        "train_pairs = text_pairs[:num_train_samples]\n",
        "val_pairs = text_pairs[num_train_samples:]\n",
        "\n",
        "print(f\"{len(text_pairs)} total pairs\")\n",
        "print(f\"{len(train_pairs)} training pairs\")\n",
        "print(f\"{len(val_pairs)} validation pairs\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 62,
      "metadata": {
        "tags": [],
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TlcqI07mwXMN",
        "outputId": "2a70af75-e39b-4698-c788-4e90c965f279"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "('the old woman fell and could not get up.', 'la anciana se cayó y no pudo levantarse.')\n",
            "('what is this the abbreviation for?', '¿de qué es abreviatura esto?')\n",
            "(\"you're not sick.\", 'no estás enferma.')\n",
            "('i have no knife to cut with.', 'no tengo un cuchillo con que cortarlo.')\n",
            "('americans admire lincoln for his honesty.', 'los estadounidenses admiran a lincoln por su honestidad.')\n"
          ]
        }
      ],
      "source": [
        "for s in train_pairs[:5]:\n",
        "    print(s)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Pipeline"
      ],
      "metadata": {
        "id": "rrZ98cykw3Vt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torchtext.data.utils import get_tokenizer\n",
        "from torchtext.vocab import vocab as Vocab\n",
        "from collections import Counter"
      ],
      "metadata": {
        "id": "k74yIcL9w2c3"
      },
      "execution_count": 63,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#!python -m spacy download en_core_web_sm\n",
        "#!python -m spacy download es_core_news_sm"
      ],
      "metadata": {
        "id": "qo7PAIyE9RyR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "712839b9-ece0-4046-8e6a-1e13d9637cf2"
      },
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting en-core-web-sm==3.7.1\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.7.1/en_core_web_sm-3.7.1-py3-none-any.whl (12.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.8/12.8 MB\u001b[0m \u001b[31m29.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: spacy<3.8.0,>=3.7.2 in /usr/local/lib/python3.10/dist-packages (from en-core-web-sm==3.7.1) (3.7.4)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.0.10)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.0.8)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.0.9)\n",
            "Requirement already satisfied: thinc<8.3.0,>=8.2.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (8.2.3)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.1.2)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.4.8)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.0.10)\n",
            "Requirement already satisfied: weasel<0.4.0,>=0.1.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.3.4)\n",
            "Requirement already satisfied: typer<0.10.0,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.9.4)\n",
            "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (6.4.0)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (4.66.2)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.31.0)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.7.0)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.1.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (67.7.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (24.0)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.3.0)\n",
            "Requirement already satisfied: numpy>=1.19.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.25.2)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.6.0)\n",
            "Requirement already satisfied: pydantic-core==2.18.1 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.18.1)\n",
            "Requirement already satisfied: typing-extensions>=4.6.1 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (4.11.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2024.2.2)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.10/dist-packages (from thinc<8.3.0,>=8.2.2->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.7.11)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from thinc<8.3.0,>=8.2.2->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.1.4)\n",
            "Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.10/dist-packages (from typer<0.10.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (8.1.7)\n",
            "Requirement already satisfied: cloudpathlib<0.17.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from weasel<0.4.0,>=0.1.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.16.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.1.5)\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('en_core_web_sm')\n",
            "\u001b[38;5;3m⚠ Restart to reload dependencies\u001b[0m\n",
            "If you are in a Jupyter or Colab notebook, you may need to restart Python in\n",
            "order to load all the package's dependencies. You can do this by selecting the\n",
            "'Restart kernel' or 'Restart runtime' option.\n",
            "Collecting es-core-news-sm==3.7.0\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/es_core_news_sm-3.7.0/es_core_news_sm-3.7.0-py3-none-any.whl (12.9 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.9/12.9 MB\u001b[0m \u001b[31m43.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: spacy<3.8.0,>=3.7.0 in /usr/local/lib/python3.10/dist-packages (from es-core-news-sm==3.7.0) (3.7.4)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (1.0.10)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (2.0.8)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (3.0.9)\n",
            "Requirement already satisfied: thinc<8.3.0,>=8.2.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (8.2.3)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (1.1.2)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (2.4.8)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (2.0.10)\n",
            "Requirement already satisfied: weasel<0.4.0,>=0.1.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (0.3.4)\n",
            "Requirement already satisfied: typer<0.10.0,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (0.9.4)\n",
            "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (6.4.0)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (4.66.2)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (2.31.0)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (2.7.0)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (3.1.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (67.7.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (24.0)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (3.3.0)\n",
            "Requirement already satisfied: numpy>=1.19.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (1.25.2)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (0.6.0)\n",
            "Requirement already satisfied: pydantic-core==2.18.1 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (2.18.1)\n",
            "Requirement already satisfied: typing-extensions>=4.6.1 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (4.11.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (2024.2.2)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.10/dist-packages (from thinc<8.3.0,>=8.2.2->spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (0.7.11)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from thinc<8.3.0,>=8.2.2->spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (0.1.4)\n",
            "Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.10/dist-packages (from typer<0.10.0,>=0.3.0->spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (8.1.7)\n",
            "Requirement already satisfied: cloudpathlib<0.17.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from weasel<0.4.0,>=0.1.0->spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (0.16.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (2.1.5)\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('es_core_news_sm')\n",
            "\u001b[38;5;3m⚠ Restart to reload dependencies\u001b[0m\n",
            "If you are in a Jupyter or Colab notebook, you may need to restart Python in\n",
            "order to load all the package's dependencies. You can do this by selecting the\n",
            "'Restart kernel' or 'Restart runtime' option.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "eng_tokenizer = get_tokenizer(\"spacy\", language=\"en_core_web_sm\")\n",
        "spa_tokenizer = get_tokenizer(\"spacy\", language=\"es_core_news_sm\")"
      ],
      "metadata": {
        "id": "XkaifFNCw7Pu"
      },
      "execution_count": 65,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def build_vocab(text, tokenizers, min_freq=5):\n",
        "    eng_tokenizer, spa_tokenizer = tokenizers\n",
        "    eng_counter = Counter()\n",
        "    spa_counter = Counter()\n",
        "    for eng_string_, spa_string_ in text:\n",
        "        eng_counter.update(eng_tokenizer(eng_string_))\n",
        "        spa_counter.update(spa_tokenizer(spa_string_))\n",
        "    eng_vocab = Vocab(eng_counter, min_freq=min_freq,\n",
        "                       specials=[\"<unk>\", \"<pad>\"])\n",
        "    spa_vocab = Vocab(spa_counter, min_freq=min_freq,\n",
        "                       specials=[\"<unk>\", \"<pad>\", \"<bos>\", \"<eos>\"])\n",
        "    return eng_vocab, spa_vocab\n",
        "\n",
        "eng_vocab, spa_vocab = build_vocab(text_pairs,\n",
        "                                   [eng_tokenizer, spa_tokenizer],\n",
        "                                   min_freq=0)"
      ],
      "metadata": {
        "id": "w1UVOFChzQxI"
      },
      "execution_count": 66,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "eng_vocab_size = len(eng_vocab)\n",
        "spa_vocab_size = len(spa_vocab)\n",
        "eng_vocab_size, spa_vocab_size"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aVxbaNuOz5zC",
        "outputId": "fc408038-2264-4d77-d9f9-0f0516af1570"
      },
      "execution_count": 67,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(13229, 26116)"
            ]
          },
          "metadata": {},
          "execution_count": 67
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "maxlen = 20\n",
        "\n",
        "def data_process(text):\n",
        "    data = []\n",
        "    for eng, spa in text:\n",
        "        eng_tensor_ = torch.tensor([eng_vocab[token] for token in eng_tokenizer(eng)],\n",
        "                                dtype=torch.long)\n",
        "        spa_tensor_ = torch.tensor([spa_vocab[token] for token in spa_tokenizer(spa)],\n",
        "                                dtype=torch.long)\n",
        "\n",
        "        if eng_tensor_.shape[0] < maxlen:\n",
        "            data.append((eng_tensor_, spa_tensor_))\n",
        "    return data\n",
        "\n",
        "train_data = data_process(train_pairs)\n",
        "val_data = data_process(val_pairs)"
      ],
      "metadata": {
        "id": "KHnx-TxW5suR"
      },
      "execution_count": 102,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 64\n",
        "PAD_IDX = eng_vocab[\"<pad>\"]\n",
        "BOS_IDX = spa_vocab[\"<bos>\"]\n",
        "EOS_IDX = spa_vocab[\"<eos>\"]\n",
        "\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "def generate_batch(data_batch):\n",
        "    x, y = [], []\n",
        "    for (x_item, y_item) in data_batch:\n",
        "        x.append(x_item)\n",
        "        y.append(torch.cat([torch.tensor([BOS_IDX]),\n",
        "                            y_item,\n",
        "                            torch.tensor([EOS_IDX])], dim=0))\n",
        "\n",
        "    x = pad_sequence(x, batch_first=True, padding_value=PAD_IDX)\n",
        "    y = pad_sequence(x, batch_first=True, padding_value=PAD_IDX)\n",
        "    return x, y ## Aquí tengo mis dudas si\n",
        "\n",
        "\n",
        "train_loader = DataLoader(train_data, batch_size=batch_size,\n",
        "                          shuffle=True, collate_fn=generate_batch,\n",
        "                          num_workers=4, pin_memory=True)\n",
        "val_loader = DataLoader(val_data, batch_size=batch_size,\n",
        "                          shuffle=True, collate_fn=generate_batch,\n",
        "                          num_workers=4, pin_memory=True)"
      ],
      "metadata": {
        "id": "HtirYaUR7rEq"
      },
      "execution_count": 103,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        " train_batch, train_target_batch = next(iter(train_loader))"
      ],
      "metadata": {
        "id": "svAmT1eK77Ui"
      },
      "execution_count": 104,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        " train_batch.shape, train_target_batch.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5sqU1qvY8fqc",
        "outputId": "b375f67a-bf1a-4d5d-ddf7-2d08ef49b21b"
      },
      "execution_count": 105,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(torch.Size([64, 19]), torch.Size([64, 19]))"
            ]
          },
          "metadata": {},
          "execution_count": 105
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Atención"
      ],
      "metadata": {
        "id": "6Wtdola5Aoz8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "from torch import optim\n",
        "import time"
      ],
      "metadata": {
        "id": "w7ZM-KEXAnMg"
      },
      "execution_count": 106,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Attention(nn.Module): ##SELF ATTENTION\n",
        "    def __init__(self, dim, maxlen, n_heads=4, bias=True):\n",
        "        super().__init__()\n",
        "        self.n_heads = n_heads\n",
        "        self.scale = (dim // n_heads) ** -0.5\n",
        "        self.qw = nn.Linear(dim, dim, bias = bias)\n",
        "        self.kw = nn.Linear(dim, dim, bias = bias)\n",
        "        self.vw = nn.Linear(dim, dim, bias = bias)\n",
        "\n",
        "        self.ow = nn.Linear(dim, dim, bias = bias)\n",
        "        self.register_buffer(\"bias\", torch.tril(torch.ones(maxlen, maxlen)).view(1, 1, maxlen, maxlen))\n",
        "\n",
        "    def forward(self, x):\n",
        "        B, L, D = x.shape\n",
        "        q = self.qw(x)\n",
        "        k = self.kw(x)\n",
        "        v = self.vw(x)\n",
        "\n",
        "        B, L, D = q.shape\n",
        "        q = torch.reshape(q, [B, L, self.n_heads, -1])\n",
        "        q = torch.permute(q, [0, 2, 1, 3])\n",
        "        k = torch.reshape(k, [B, L, self.n_heads, -1])\n",
        "        k = torch.permute(k, [0, 2, 3, 1])\n",
        "        v = torch.reshape(v, [B, L, self.n_heads, -1])\n",
        "        v = torch.permute(v, [0, 2, 1, 3])\n",
        "\n",
        "        qk = torch.matmul(q, k) * self.scale\n",
        "        qk = qk.masked_fill(self.bias[:,:,:L,:L] == 0, float('-inf'))\n",
        "\n",
        "        attn = torch.softmax(qk, dim=-1)\n",
        "\n",
        "        v_attn = torch.matmul(attn, v)\n",
        "        v_attn = torch.permute(v_attn, [0, 2, 1, 3])\n",
        "        v_attn = torch.reshape(v_attn, [B, L, D])\n",
        "\n",
        "        x = self.ow(v_attn)\n",
        "        return x\n",
        "\n",
        "\n",
        "test_layer = Attention(32, maxlen, n_heads=1)\n",
        "test_layer(torch.ones([1, maxlen, 32]))"
      ],
      "metadata": {
        "id": "2aUpHphMIntX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "52ce650b-da30-41fd-8185-f0b80965ff47"
      },
      "execution_count": 107,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[[-0.0179,  0.6233, -0.4003,  0.1876,  0.2364,  0.2508, -0.0267,\n",
              "          -0.0562, -0.1740,  0.2570, -0.3895,  0.2590, -0.1715,  0.0528,\n",
              "          -0.1821, -0.4856, -0.0017, -0.2047,  0.1021, -0.2731, -0.0652,\n",
              "          -0.1780, -0.1414, -0.1646,  0.1789,  0.1220,  0.2947,  0.4903,\n",
              "          -0.0541, -0.2997, -0.1387,  0.1766],\n",
              "         [-0.0179,  0.6233, -0.4003,  0.1876,  0.2364,  0.2508, -0.0267,\n",
              "          -0.0562, -0.1740,  0.2570, -0.3895,  0.2590, -0.1715,  0.0528,\n",
              "          -0.1821, -0.4856, -0.0017, -0.2047,  0.1021, -0.2731, -0.0652,\n",
              "          -0.1780, -0.1414, -0.1646,  0.1789,  0.1220,  0.2947,  0.4903,\n",
              "          -0.0541, -0.2997, -0.1387,  0.1766],\n",
              "         [-0.0179,  0.6233, -0.4003,  0.1876,  0.2364,  0.2508, -0.0267,\n",
              "          -0.0562, -0.1740,  0.2570, -0.3895,  0.2590, -0.1715,  0.0528,\n",
              "          -0.1821, -0.4856, -0.0017, -0.2047,  0.1021, -0.2731, -0.0652,\n",
              "          -0.1780, -0.1414, -0.1646,  0.1789,  0.1220,  0.2947,  0.4903,\n",
              "          -0.0541, -0.2997, -0.1387,  0.1766],\n",
              "         [-0.0179,  0.6233, -0.4003,  0.1876,  0.2364,  0.2508, -0.0267,\n",
              "          -0.0562, -0.1740,  0.2570, -0.3895,  0.2590, -0.1715,  0.0528,\n",
              "          -0.1821, -0.4856, -0.0017, -0.2047,  0.1021, -0.2731, -0.0652,\n",
              "          -0.1780, -0.1414, -0.1646,  0.1789,  0.1220,  0.2947,  0.4903,\n",
              "          -0.0541, -0.2997, -0.1387,  0.1766],\n",
              "         [-0.0179,  0.6233, -0.4003,  0.1876,  0.2364,  0.2508, -0.0267,\n",
              "          -0.0562, -0.1740,  0.2570, -0.3895,  0.2590, -0.1715,  0.0528,\n",
              "          -0.1821, -0.4856, -0.0017, -0.2047,  0.1021, -0.2731, -0.0652,\n",
              "          -0.1780, -0.1414, -0.1646,  0.1789,  0.1220,  0.2947,  0.4903,\n",
              "          -0.0541, -0.2997, -0.1387,  0.1766],\n",
              "         [-0.0179,  0.6233, -0.4003,  0.1876,  0.2364,  0.2508, -0.0267,\n",
              "          -0.0562, -0.1740,  0.2570, -0.3895,  0.2590, -0.1715,  0.0528,\n",
              "          -0.1821, -0.4856, -0.0017, -0.2047,  0.1021, -0.2731, -0.0652,\n",
              "          -0.1780, -0.1414, -0.1646,  0.1789,  0.1220,  0.2947,  0.4903,\n",
              "          -0.0541, -0.2997, -0.1387,  0.1766],\n",
              "         [-0.0179,  0.6233, -0.4003,  0.1876,  0.2364,  0.2508, -0.0267,\n",
              "          -0.0562, -0.1740,  0.2570, -0.3895,  0.2590, -0.1715,  0.0528,\n",
              "          -0.1821, -0.4856, -0.0017, -0.2047,  0.1021, -0.2731, -0.0652,\n",
              "          -0.1780, -0.1414, -0.1646,  0.1789,  0.1220,  0.2947,  0.4903,\n",
              "          -0.0541, -0.2997, -0.1387,  0.1766],\n",
              "         [-0.0179,  0.6233, -0.4003,  0.1876,  0.2364,  0.2508, -0.0267,\n",
              "          -0.0562, -0.1740,  0.2570, -0.3895,  0.2590, -0.1715,  0.0528,\n",
              "          -0.1821, -0.4856, -0.0017, -0.2047,  0.1021, -0.2731, -0.0652,\n",
              "          -0.1780, -0.1414, -0.1646,  0.1789,  0.1220,  0.2947,  0.4903,\n",
              "          -0.0541, -0.2997, -0.1387,  0.1766],\n",
              "         [-0.0179,  0.6233, -0.4003,  0.1876,  0.2364,  0.2508, -0.0267,\n",
              "          -0.0562, -0.1740,  0.2570, -0.3895,  0.2590, -0.1715,  0.0528,\n",
              "          -0.1821, -0.4856, -0.0017, -0.2047,  0.1021, -0.2731, -0.0652,\n",
              "          -0.1780, -0.1414, -0.1646,  0.1789,  0.1220,  0.2947,  0.4903,\n",
              "          -0.0541, -0.2997, -0.1387,  0.1766],\n",
              "         [-0.0179,  0.6233, -0.4003,  0.1876,  0.2364,  0.2508, -0.0267,\n",
              "          -0.0562, -0.1740,  0.2570, -0.3895,  0.2590, -0.1715,  0.0528,\n",
              "          -0.1821, -0.4856, -0.0017, -0.2047,  0.1021, -0.2731, -0.0652,\n",
              "          -0.1780, -0.1414, -0.1646,  0.1789,  0.1220,  0.2947,  0.4903,\n",
              "          -0.0541, -0.2997, -0.1387,  0.1766],\n",
              "         [-0.0179,  0.6233, -0.4003,  0.1876,  0.2364,  0.2508, -0.0267,\n",
              "          -0.0562, -0.1740,  0.2570, -0.3895,  0.2590, -0.1715,  0.0528,\n",
              "          -0.1821, -0.4856, -0.0017, -0.2047,  0.1021, -0.2731, -0.0652,\n",
              "          -0.1780, -0.1414, -0.1646,  0.1789,  0.1220,  0.2947,  0.4903,\n",
              "          -0.0541, -0.2997, -0.1387,  0.1766],\n",
              "         [-0.0179,  0.6233, -0.4003,  0.1876,  0.2364,  0.2508, -0.0267,\n",
              "          -0.0562, -0.1740,  0.2570, -0.3895,  0.2590, -0.1715,  0.0528,\n",
              "          -0.1821, -0.4856, -0.0017, -0.2047,  0.1021, -0.2731, -0.0652,\n",
              "          -0.1780, -0.1414, -0.1646,  0.1789,  0.1220,  0.2947,  0.4903,\n",
              "          -0.0541, -0.2997, -0.1387,  0.1766],\n",
              "         [-0.0179,  0.6233, -0.4003,  0.1876,  0.2364,  0.2508, -0.0267,\n",
              "          -0.0562, -0.1740,  0.2570, -0.3895,  0.2590, -0.1715,  0.0528,\n",
              "          -0.1821, -0.4856, -0.0017, -0.2047,  0.1021, -0.2731, -0.0652,\n",
              "          -0.1780, -0.1414, -0.1646,  0.1789,  0.1220,  0.2947,  0.4903,\n",
              "          -0.0541, -0.2997, -0.1387,  0.1766],\n",
              "         [-0.0179,  0.6233, -0.4003,  0.1876,  0.2364,  0.2508, -0.0267,\n",
              "          -0.0562, -0.1740,  0.2570, -0.3895,  0.2590, -0.1715,  0.0528,\n",
              "          -0.1821, -0.4856, -0.0017, -0.2047,  0.1021, -0.2731, -0.0652,\n",
              "          -0.1780, -0.1414, -0.1646,  0.1789,  0.1220,  0.2947,  0.4903,\n",
              "          -0.0541, -0.2997, -0.1387,  0.1766],\n",
              "         [-0.0179,  0.6233, -0.4003,  0.1876,  0.2364,  0.2508, -0.0267,\n",
              "          -0.0562, -0.1740,  0.2570, -0.3895,  0.2590, -0.1715,  0.0528,\n",
              "          -0.1821, -0.4856, -0.0017, -0.2047,  0.1021, -0.2731, -0.0652,\n",
              "          -0.1780, -0.1414, -0.1646,  0.1789,  0.1220,  0.2947,  0.4903,\n",
              "          -0.0541, -0.2997, -0.1387,  0.1766],\n",
              "         [-0.0179,  0.6233, -0.4003,  0.1876,  0.2364,  0.2508, -0.0267,\n",
              "          -0.0562, -0.1740,  0.2570, -0.3895,  0.2590, -0.1715,  0.0528,\n",
              "          -0.1821, -0.4856, -0.0017, -0.2047,  0.1021, -0.2731, -0.0652,\n",
              "          -0.1780, -0.1414, -0.1646,  0.1789,  0.1220,  0.2947,  0.4903,\n",
              "          -0.0541, -0.2997, -0.1387,  0.1766],\n",
              "         [-0.0179,  0.6233, -0.4003,  0.1876,  0.2364,  0.2508, -0.0267,\n",
              "          -0.0562, -0.1740,  0.2570, -0.3895,  0.2590, -0.1715,  0.0528,\n",
              "          -0.1821, -0.4856, -0.0017, -0.2047,  0.1021, -0.2731, -0.0652,\n",
              "          -0.1780, -0.1414, -0.1646,  0.1789,  0.1220,  0.2947,  0.4903,\n",
              "          -0.0541, -0.2997, -0.1387,  0.1766],\n",
              "         [-0.0179,  0.6233, -0.4003,  0.1876,  0.2364,  0.2508, -0.0267,\n",
              "          -0.0562, -0.1740,  0.2570, -0.3895,  0.2590, -0.1715,  0.0528,\n",
              "          -0.1821, -0.4856, -0.0017, -0.2047,  0.1021, -0.2731, -0.0652,\n",
              "          -0.1780, -0.1414, -0.1646,  0.1789,  0.1220,  0.2947,  0.4903,\n",
              "          -0.0541, -0.2997, -0.1387,  0.1766],\n",
              "         [-0.0179,  0.6233, -0.4003,  0.1876,  0.2364,  0.2508, -0.0267,\n",
              "          -0.0562, -0.1740,  0.2570, -0.3895,  0.2590, -0.1715,  0.0528,\n",
              "          -0.1821, -0.4856, -0.0017, -0.2047,  0.1021, -0.2731, -0.0652,\n",
              "          -0.1780, -0.1414, -0.1646,  0.1789,  0.1220,  0.2947,  0.4903,\n",
              "          -0.0541, -0.2997, -0.1387,  0.1766],\n",
              "         [-0.0179,  0.6233, -0.4003,  0.1876,  0.2364,  0.2508, -0.0267,\n",
              "          -0.0562, -0.1740,  0.2570, -0.3895,  0.2590, -0.1715,  0.0528,\n",
              "          -0.1821, -0.4856, -0.0017, -0.2047,  0.1021, -0.2731, -0.0652,\n",
              "          -0.1780, -0.1414, -0.1646,  0.1789,  0.1220,  0.2947,  0.4903,\n",
              "          -0.0541, -0.2997, -0.1387,  0.1766]]], grad_fn=<ViewBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 107
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "class CrossAttention(nn.Module): ##CROSS ATTENTION\n",
        "    def __init__(self, dim, maxlen, n_heads=4, bias=True):\n",
        "        super().__init__()\n",
        "        self.n_heads = n_heads\n",
        "        self.scale = (dim // n_heads) ** -0.5\n",
        "        self.qw = nn.Linear(dim, dim, bias=bias)\n",
        "        self.kw = nn.Linear(dim, dim, bias=bias)\n",
        "        self.vw = nn.Linear(dim, dim, bias=bias)\n",
        "\n",
        "        self.ow = nn.Linear(dim, dim, bias=bias)\n",
        "        self.register_buffer(\"bias\", torch.tril(torch.ones(maxlen, maxlen)).view(1, 1, maxlen, maxlen))\n",
        "\n",
        "    def forward(self, x): # k y v son la frase en inglés. Contexto\n",
        "\n",
        "        #print(len(x.shape), \"Cross\")\n",
        "\n",
        "        #if (len(x.shape) < 3):\n",
        "        #  x = x.unsqueeze(0)\n",
        "\n",
        "        B, L, D = x.shape\n",
        "        q = self.qw(x)\n",
        "        k = self.kw(x)\n",
        "        v = self.vw(x)\n",
        "\n",
        "        B, L_context, D = k.shape\n",
        "        q = torch.reshape(q, [B, L, self.n_heads, -1])\n",
        "        q = torch.permute(q, [0, 2, 1, 3])\n",
        "        k = torch.reshape(k, [B, L_context, self.n_heads, -1])\n",
        "        k = torch.permute(k, [0, 2, 3, 1])\n",
        "        v = torch.reshape(v, [B, L_context, self.n_heads, -1])\n",
        "        v = torch.permute(v, [0, 2, 1, 3])\n",
        "\n",
        "        qk = torch.matmul(q, k) * self.scale\n",
        "        qk = qk.masked_fill(self.bias[:, :, :L, :L_context] == 0, float('-inf'))\n",
        "\n",
        "        attn = torch.softmax(qk, dim=-1)\n",
        "\n",
        "        v_attn = torch.matmul(attn, v)\n",
        "        v_attn = torch.permute(v_attn, [0, 2, 1, 3])\n",
        "        v_attn = torch.reshape(v_attn, [B, L, D])\n",
        "\n",
        "        x = self.ow(v_attn)\n",
        "        return x\n",
        "\n",
        "#test_layer = CrossAttention(32, maxlen, n_heads=1)\n",
        "#inputs = torch.ones([ maxlen, 32])\n",
        "#context = torch.ones([ maxlen-6, 32])\n",
        "#context2 = torch.ones([ maxlen-6, 32])\n",
        "#test_layer([inputs, context])\n"
      ],
      "metadata": {
        "id": "U4M-Ccj8iZie"
      },
      "execution_count": 108,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Transformer\n"
      ],
      "metadata": {
        "id": "I4Pb9-hlkyCE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Transformer(nn.Module):\n",
        "    def __init__(self, dim, maxlen, heads=4, mlp_dim=512, rate=0.0):\n",
        "        super().__init__()\n",
        "        self.ln_1 = nn.LayerNorm(dim)\n",
        "        self.c_attn = CrossAttention(dim, maxlen)\n",
        "        self.ln_2 = nn.LayerNorm(dim)\n",
        "        self.attn = Attention(dim, maxlen)\n",
        "        self.ln_3 = nn.LayerNorm(dim)\n",
        "        self.mlp = nn.Sequential(\n",
        "            nn.Linear(dim, mlp_dim),\n",
        "            nn.GELU(),\n",
        "            nn.Dropout(rate),\n",
        "            nn.Linear(mlp_dim, dim),\n",
        "            nn.Dropout(rate),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.c_attn(self.ln_1(x)) + x\n",
        "        x = self.attn(self.ln_2(x)) + x\n",
        "        return self.mlp(self.ln_3(x)) + x\n",
        "\n",
        "\n",
        "#test_layer = Transformer(32, maxlen)\n",
        "#test_layer([torch.ones([ maxlen, 32]),torch.ones([ maxlen-6, 32])]).shape"
      ],
      "metadata": {
        "id": "Pv0_JhpXk0iY"
      },
      "execution_count": 109,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_batch.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LA5JoHXZmR2W",
        "outputId": "675ea50c-773e-40ee-ab71-a6fc271a3314"
      },
      "execution_count": 110,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([64, 19])"
            ]
          },
          "metadata": {},
          "execution_count": 110
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class GPT(nn.Module):\n",
        "    def __init__(self, dim, vocab_size_spa, vocab_size_eng, maxlen, depth=3,\n",
        "                 mlp_dim=512, rate=0.2):\n",
        "        super().__init__()\n",
        "        self.embedding_spa = nn.Embedding(vocab_size_spa, dim)\n",
        "        self.pos_embedding_spa = nn.Parameter(\n",
        "            torch.randn(1, maxlen, dim))\n",
        "        self.embedding_eng = nn.Embedding(vocab_size_spa, dim)\n",
        "        self.pos_embedding_eng = nn.Parameter(\n",
        "            torch.randn(1, maxlen, dim))\n",
        "\n",
        "        self.transformer = nn.Sequential()\n",
        "        for _ in range(depth):\n",
        "            self.transformer.append(Transformer(dim, maxlen))\n",
        "\n",
        "        self.head = nn.Linear(dim, vocab_size_eng, bias=False)\n",
        "\n",
        "    def forward(self, x):\n",
        "        Bx, Lx = x.shape\n",
        "        #By, Ly = y.shape\n",
        "        x = self.embedding_eng(x)\n",
        "        x += self.pos_embedding_eng[:, :Lx]\n",
        "        #y = self.embedding_spa(y)\n",
        "        #y += self.pos_embedding_spa[:, :Ly]\n",
        "        x = self.transformer(x)\n",
        "        x = self.head(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "model_dim = 128\n",
        "depth = 3\n",
        "mlp_dim = 128\n",
        "\n",
        "gpt = GPT(dim=model_dim, vocab_size_spa=spa_vocab_size, vocab_size_eng = eng_vocab_size,\n",
        "          maxlen=maxlen, depth=depth, mlp_dim=mlp_dim)\n",
        "output = gpt(train_batch)\n",
        "output.shape, train_target_batch.shape\n",
        "\n"
      ],
      "metadata": {
        "id": "mEbB6pNKmy4s",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4d29e08c-0ac2-44be-e6f5-7a4dcf817c8f"
      },
      "execution_count": 111,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(torch.Size([64, 19, 13229]), torch.Size([64, 19]))"
            ]
          },
          "metadata": {},
          "execution_count": 111
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(output[1])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Tp2XU2qMHgR4",
        "outputId": "e18bbd94-9440-4afd-e848-b88b339ab635"
      },
      "execution_count": 112,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[ 0.4592, -1.0247,  0.9509,  ..., -0.6123,  0.6992,  0.5538],\n",
            "        [ 0.5280,  0.5906, -1.0630,  ...,  0.4759,  1.3766,  0.0219],\n",
            "        [-0.2824,  0.6400, -0.2289,  ...,  1.0530,  1.5233,  2.0520],\n",
            "        ...,\n",
            "        [-0.1289,  0.2473,  0.4927,  ..., -1.5995,  0.4709,  0.9666],\n",
            "        [-0.4612,  0.8273,  0.1108,  ..., -2.4501,  0.9107,  0.7009],\n",
            "        [-0.2550,  0.3324,  0.5564,  ..., -1.6969, -0.0230,  0.0172]],\n",
            "       grad_fn=<SelectBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Entrenamiento"
      ],
      "metadata": {
        "id": "jTKV_znlmqhK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
        "print(device)\n",
        "\n",
        "gpt.to(device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I0nM-uUzmsRl",
        "outputId": "e3722b95-13c2-4a99-f66f-b3d464390875"
      },
      "execution_count": 113,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cpu\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "GPT(\n",
              "  (embedding_spa): Embedding(26116, 128)\n",
              "  (embedding_eng): Embedding(26116, 128)\n",
              "  (transformer): Sequential(\n",
              "    (0): Transformer(\n",
              "      (ln_1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
              "      (c_attn): CrossAttention(\n",
              "        (qw): Linear(in_features=128, out_features=128, bias=True)\n",
              "        (kw): Linear(in_features=128, out_features=128, bias=True)\n",
              "        (vw): Linear(in_features=128, out_features=128, bias=True)\n",
              "        (ow): Linear(in_features=128, out_features=128, bias=True)\n",
              "      )\n",
              "      (ln_2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
              "      (attn): Attention(\n",
              "        (qw): Linear(in_features=128, out_features=128, bias=True)\n",
              "        (kw): Linear(in_features=128, out_features=128, bias=True)\n",
              "        (vw): Linear(in_features=128, out_features=128, bias=True)\n",
              "        (ow): Linear(in_features=128, out_features=128, bias=True)\n",
              "      )\n",
              "      (ln_3): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
              "      (mlp): Sequential(\n",
              "        (0): Linear(in_features=128, out_features=512, bias=True)\n",
              "        (1): GELU(approximate='none')\n",
              "        (2): Dropout(p=0.0, inplace=False)\n",
              "        (3): Linear(in_features=512, out_features=128, bias=True)\n",
              "        (4): Dropout(p=0.0, inplace=False)\n",
              "      )\n",
              "    )\n",
              "    (1): Transformer(\n",
              "      (ln_1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
              "      (c_attn): CrossAttention(\n",
              "        (qw): Linear(in_features=128, out_features=128, bias=True)\n",
              "        (kw): Linear(in_features=128, out_features=128, bias=True)\n",
              "        (vw): Linear(in_features=128, out_features=128, bias=True)\n",
              "        (ow): Linear(in_features=128, out_features=128, bias=True)\n",
              "      )\n",
              "      (ln_2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
              "      (attn): Attention(\n",
              "        (qw): Linear(in_features=128, out_features=128, bias=True)\n",
              "        (kw): Linear(in_features=128, out_features=128, bias=True)\n",
              "        (vw): Linear(in_features=128, out_features=128, bias=True)\n",
              "        (ow): Linear(in_features=128, out_features=128, bias=True)\n",
              "      )\n",
              "      (ln_3): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
              "      (mlp): Sequential(\n",
              "        (0): Linear(in_features=128, out_features=512, bias=True)\n",
              "        (1): GELU(approximate='none')\n",
              "        (2): Dropout(p=0.0, inplace=False)\n",
              "        (3): Linear(in_features=512, out_features=128, bias=True)\n",
              "        (4): Dropout(p=0.0, inplace=False)\n",
              "      )\n",
              "    )\n",
              "    (2): Transformer(\n",
              "      (ln_1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
              "      (c_attn): CrossAttention(\n",
              "        (qw): Linear(in_features=128, out_features=128, bias=True)\n",
              "        (kw): Linear(in_features=128, out_features=128, bias=True)\n",
              "        (vw): Linear(in_features=128, out_features=128, bias=True)\n",
              "        (ow): Linear(in_features=128, out_features=128, bias=True)\n",
              "      )\n",
              "      (ln_2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
              "      (attn): Attention(\n",
              "        (qw): Linear(in_features=128, out_features=128, bias=True)\n",
              "        (kw): Linear(in_features=128, out_features=128, bias=True)\n",
              "        (vw): Linear(in_features=128, out_features=128, bias=True)\n",
              "        (ow): Linear(in_features=128, out_features=128, bias=True)\n",
              "      )\n",
              "      (ln_3): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
              "      (mlp): Sequential(\n",
              "        (0): Linear(in_features=128, out_features=512, bias=True)\n",
              "        (1): GELU(approximate='none')\n",
              "        (2): Dropout(p=0.0, inplace=False)\n",
              "        (3): Linear(in_features=512, out_features=128, bias=True)\n",
              "        (4): Dropout(p=0.0, inplace=False)\n",
              "      )\n",
              "    )\n",
              "  )\n",
              "  (head): Linear(in_features=128, out_features=13229, bias=False)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 113
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "PAD_IDX = spa_vocab.get_stoi()['<pad>']\n",
        "PAD_IDX"
      ],
      "metadata": {
        "id": "uSRB-kqQ0Z57",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "65d367bb-de13-4777-ec7c-e476b5c1efeb"
      },
      "execution_count": 114,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1"
            ]
          },
          "metadata": {},
          "execution_count": 114
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "optimizer = optim.Adam(gpt.parameters(), lr=0.001)\n",
        "loss_fn = torch.nn.CrossEntropyLoss(ignore_index=PAD_IDX)"
      ],
      "metadata": {
        "id": "mAH0OIf00asK"
      },
      "execution_count": 115,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train(model, device, train_loader, optimizer, epoch):\n",
        "    start = time.time()\n",
        "    running_loss = 0.0\n",
        "    model.train()\n",
        "    for inputs, targets in train_loader:\n",
        "        targets = targets.view(-1)\n",
        "        inputs, targets = inputs.to(device), targets.to(device)\n",
        "\n",
        "        # zero the parameter gradients\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # forward + backward + optimize\n",
        "        outputs = model(inputs)\n",
        "        outputs = outputs.view(-1, outputs.size(-1))\n",
        "        loss = loss_fn(outputs, targets)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += loss.item()\n",
        "\n",
        "    print(f'\\nTime for epoch {epoch} is {time.time()-start:4f} sec Train loss: {running_loss / len(train_loader):4f}')"
      ],
      "metadata": {
        "id": "7LpeVUem0dZV"
      },
      "execution_count": 116,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def translate(model, sentence, device, maxlen):\n",
        "    with torch.no_grad():\n",
        "        model.eval()\n",
        "        idx = torch.tensor([eng_vocab[token] for token in eng_tokenizer(sentence)],\n",
        "                                    dtype=torch.long)\n",
        "        idx = idx.reshape([1, -1])\n",
        "        maxlen = maxlen - idx.shape[-1]\n",
        "\n",
        "        for _ in range(maxlen):\n",
        "            idx = idx.to(device)\n",
        "            logits = gpt(idx)[:, -1, :]\n",
        "            probs = torch.softmax(logits, dim=-1)\n",
        "\n",
        "            _, idx_next = torch.topk(probs, k=1, dim=-1)\n",
        "            idx = torch.cat((idx, idx_next), dim=1)\n",
        "\n",
        "        txt = \" \".join(\n",
        "                    [eng_vocab.get_itos()[idx[0, _]] for _ in range(maxlen)]\n",
        "                )\n",
        "    return txt.replace(\"<eos>\", \"\")\n",
        "\n",
        "sentences = [\"good morning\",\n",
        "             \"i hate mondays\"]\n",
        "\n",
        "for s in sentences:\n",
        "    trans = translate(gpt, s, device, maxlen)\n",
        "    print(f\"\\n{trans}\")"
      ],
      "metadata": {
        "id": "uzv0zMlw0u-6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ae277cf3-66a9-4267-849c-f10af2fb5427"
      },
      "execution_count": 117,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "good morning dried withdraw rowboat apologise sting korean herself heat encouragingly judo leaning pathetic rheumatism vanished driver rights\n",
            "\n",
            "i hate mondays wring valise abated aunts sleeves lasted directed lighted ambitions bone corn provoke splints rake\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "epochs = 6\n",
        "\n",
        "for epoch in range(epochs):\n",
        "    train(gpt, device, train_loader, optimizer, epoch)\n",
        "\n",
        "    # Translate test sentences\n",
        "    for s in sentences:\n",
        "        trans = translate(gpt, s, device, maxlen)\n",
        "        print(trans)"
      ],
      "metadata": {
        "id": "M5obVVPTBI-e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FJPBwpVXwXMO"
      },
      "source": [
        "## 2.- Evaluación"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [],
        "id": "HvJzBX8lwXMQ"
      },
      "outputs": [],
      "source": [
        "import warnings\n",
        "# Disable warnings\n",
        "warnings.filterwarnings(\"ignore\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [],
        "id": "TpEygBFUwXMQ"
      },
      "outputs": [],
      "source": [
        "import nltk\n",
        "\n",
        "# Lista de oraciones de referencia (lista de listas)\n",
        "referencias = [[\"El\", \"gato\", \"está\", \"en\", \"la\", \"alfombra\"],\n",
        "               [\"El\", \"perro\", \"juega\", \"en\", \"el\", \"parque\"],\n",
        "               [\"El\", \"cielo\", \"está\", \"despejado\"],\n",
        "               [\"El\", \"sol\", \"brilla\", \"intensamente\"],\n",
        "               [\"Los\", \"pájaros\", \"cantan\", \"en\", \"los\", \"árboles\"]]\n",
        "\n",
        "# Lista de oraciones candidatas (lista de listas)\n",
        "candidatas = [[\"El\", \"gato\", \"está\", \"durmiendo\", \"en\", \"la\", \"alfombra\"],\n",
        "              [\"El\", \"perro\", \"juega\", \"en\", \"el\", \"jardín\"],\n",
        "              [\"El\", \"cielo\", \"está\", \"soleado\"],\n",
        "              [\"El\", \"sol\", \"brilla\", \"intensamente\"],\n",
        "              [\"Los\", \"pájaros\", \"trinan\", \"en\", \"los\", \"árboles\"]]\n",
        "\n",
        "# Calcular el BLEU score para cada oración candidata\n",
        "for i in range(len(candidatas)):\n",
        "    referencia = referencias[i]\n",
        "    candidata = candidatas[i]\n",
        "\n",
        "    bleu_score = nltk.translate.bleu_score.sentence_bleu([referencia], candidata)\n",
        "    print(f\"BLEU score para la oración {i+1}: {bleu_score}\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.18"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}